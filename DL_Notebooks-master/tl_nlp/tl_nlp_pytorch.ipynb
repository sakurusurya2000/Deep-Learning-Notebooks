{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tl_nlp_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GT8aPqGO6Fsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning in NLP\n",
        "\n",
        "\n",
        "In this notebook, we will go through basics of Transfer Learning in NLP using  BERT and also compare the results of custom embeddings on LSTM, BiLSTM, GRU with BERT on [Twitter US Airline Sentiment dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).\n",
        "\n",
        "### Dataset\n",
        "\n",
        "Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").  It contains whether the sentiment of the tweets in this set was positive, neutral, or negative for six US airlines\n",
        "\n",
        "\n",
        "Here we will use [PyTorch](https://pytorch.org/tutorials/  \"PyTorch Tutorial\").\n",
        "\n",
        "\n",
        "Everything is explained in-detail in [blog post](https://dudeperf3ct.github.io/nlp/transfer/learning/2019/02/22/Power-of-Transfer-Learning-in-NLP/). This is notebook which replicates the result of blog and runs in colab. Enjoy!\n",
        "\n",
        "\n",
        "#### Run in Colab\n",
        "\n",
        "You can run this notebook in google colab.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dudeperf3ct/DL_notebooks/blob/master/tl_nlp/tl_nlp_pytorch.ipynb)\n",
        "\n",
        "\n",
        "### Results\n",
        "\n",
        "|  Approach | Epoch  | Time (sec)  | Train Accuracy(%)  | Dev Accuracy (%)  |\n",
        "|---|---|---|---|---|\n",
        "| LSTM  |  10  | 25  |  98 |  78.8 |\n",
        "| BiLSTM |  10 |  35 |  98 | 79.1  |\n",
        "| GRU  |  10 |  27 |  92 | 79.3  |\n",
        "| BERT  |  3 | 600  |  - | 85.03  |\n"
      ]
    },
    {
      "metadata": {
        "id": "2OtB6BOc61Jq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download pytorch pretrained"
      ]
    },
    {
      "metadata": {
        "id": "9WZVsIhn6FIa",
        "colab_type": "code",
        "outputId": "31dcab63-4556-4c7e-f10e-09f5cc3b5109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1021
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy ftfy==4.4.3\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Collecting ftfy==4.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Collecting numpy>=1.15.0 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy==4.4.3) (0.1.7)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy==4.4.3) (0.5.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
            "Successfully built ftfy\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: ftfy, numpy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed ftfy-4.4.3 numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zKIX4sfLXU6I",
        "colab_type": "code",
        "outputId": "88dbb31b-115b-4fa2-c634-9836f07a73bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install pytorch_pretrained_bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.115)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2018.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.1.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.16.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.115 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.115)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.115->boto3->pytorch_pretrained_bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WNALNFGs6-MT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Twitter Sentiment data\n",
        "\n",
        "Code Adapted from : [Link](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py)\n",
        "\n",
        "Pretrained Model : [Link](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
        "\n",
        "Paper GPT2: [Link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "Paper BERT: [Link](https://arxiv.org/pdf/1810.04805.pdf)\n"
      ]
    },
    {
      "metadata": {
        "id": "lUSiBICm7BUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b8a1716-b715-4a5b-e3f1-2bed2967930f"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "torch.manual_seed(11)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print ('Training on...', device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on... cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D6g6IL-d6FLv",
        "colab_type": "code",
        "outputId": "f136c75e-fa29-4c83-bffc-b882269772e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://query.data.world/s/hus7zihvuo5vt65cnv4fcfn2ppfj6y', encoding = \"ISO-8859-1\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_unit_id</th>\n",
              "      <th>_golden</th>\n",
              "      <th>_unit_state</th>\n",
              "      <th>_trusted_judgments</th>\n",
              "      <th>_last_judgment_at</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment:confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason:confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>681448150</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>2/25/15 5:24</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2/24/15 11:35</td>\n",
              "      <td>5.703060e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>681448153</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>2/25/15 1:53</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2/24/15 11:15</td>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>681448156</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>2/25/15 10:01</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2/24/15 11:15</td>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>681448158</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>2/25/15 3:05</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2/24/15 11:15</td>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>681448159</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>2/25/15 5:50</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2/24/15 11:14</td>\n",
              "      <td>5.703010e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
              "0  681448150    False   finalized                   3      2/25/15 5:24   \n",
              "1  681448153    False   finalized                   3      2/25/15 1:53   \n",
              "2  681448156    False   finalized                   3     2/25/15 10:01   \n",
              "3  681448158    False   finalized                   3      2/25/15 3:05   \n",
              "4  681448159    False   finalized                   3      2/25/15 5:50   \n",
              "\n",
              "  airline_sentiment  airline_sentiment:confidence negativereason  \\\n",
              "0           neutral                        1.0000            NaN   \n",
              "1          positive                        0.3486            NaN   \n",
              "2           neutral                        0.6837            NaN   \n",
              "3          negative                        1.0000     Bad Flight   \n",
              "4          negative                        1.0000     Can't Tell   \n",
              "\n",
              "   negativereason:confidence         airline airline_sentiment_gold  \\\n",
              "0                        NaN  Virgin America                    NaN   \n",
              "1                     0.0000  Virgin America                    NaN   \n",
              "2                        NaN  Virgin America                    NaN   \n",
              "3                     0.7033  Virgin America                    NaN   \n",
              "4                     1.0000  Virgin America                    NaN   \n",
              "\n",
              "         name negativereason_gold  retweet_count  \\\n",
              "0     cairdin                 NaN              0   \n",
              "1    jnardino                 NaN              0   \n",
              "2  yvonnalynn                 NaN              0   \n",
              "3    jnardino                 NaN              0   \n",
              "4    jnardino                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
              "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
              "\n",
              "   tweet_created      tweet_id tweet_location               user_timezone  \n",
              "0  2/24/15 11:35  5.703060e+17            NaN  Eastern Time (US & Canada)  \n",
              "1  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
              "2  2/24/15 11:15  5.703010e+17      Lets Play  Central Time (US & Canada)  \n",
              "3  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
              "4  2/24/15 11:14  5.703010e+17            NaN  Pacific Time (US & Canada)  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "YZK4GWZbntms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df[['text', 'airline_sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-DZCsnwsTCG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train, df_split = train_test_split(df, test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcNqfWFxt8FQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_val, df_test = train_test_split(df_split, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ASrj4Wc8NsF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.mkdir('data')\n",
        "df_train.to_csv('data/train.csv', index=False)\n",
        "df_val.to_csv('data/val.csv', index=False)\n",
        "df_test.to_csv('data/test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jRMIJG3-nrrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a09dcc86-bd2c-41ce-ed93-da05571e42bc"
      },
      "cell_type": "code",
      "source": [
        "print (df_train.shape, df_val.shape, df_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13908, 2) (366, 2) (366, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1thIJ65BASNb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "4wypRea2pYZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.Field(sequential=False, unk_token=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkT8t_MmpZB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = torchtext.data.TabularDataset.splits(path='data/',\n",
        "                                                            train='train.csv',\n",
        "                                                            validation='val.csv',\n",
        "                                                            test='test.csv',\n",
        "                                                            format='csv',\n",
        "                                                            skip_header=True,\n",
        "                                                            fields=[('text', TEXT), ('airline_sentiment', LABEL)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAC0kTn2513w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=15000)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wfxqqDe08R8U",
        "colab_type": "code",
        "outputId": "08c1ba85-56b0-4e16-97c1-f69b78a88ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "print('Unique tokens in TEXT vocabulary:', len(TEXT.vocab))\n",
        "print('Unique tokens in LABEL vocabulary:',len(LABEL.vocab))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 15002\n",
            "Unique tokens in LABEL vocabulary: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQNNaLqc8ffw",
        "colab_type": "code",
        "outputId": "1c9fb5a1-8502-4ec2-ca43-169006a613ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 12857), ('to', 8096), ('I', 6023), ('the', 5506), ('!', 4974), ('?', 4382), ('a', 4121), (',', 3931), ('you', 3753), ('for', 3752), ('@united', 3518), ('on', 3507), ('#', 3384), ('and', 3381), ('flight', 2998), ('my', 2881), ('@USAirways', 2743), ('@AmericanAir', 2731), ('is', 2607), ('in', 2414)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PatGqW_s8kk1",
        "colab_type": "code",
        "outputId": "6c67fb50-8419-49ca-e9bd-bdabcd9f372c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '.', 'to', 'I', 'the', '!', '?', 'a', ',']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9fwOT2ff8yk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort = False,\n",
        "    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CEkfHeuD3YRz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom Embedding Layer using LSTM"
      ]
    },
    {
      "metadata": {
        "id": "ggocjCdV8-Qk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "\n",
        "class LSTM_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
        "      \n",
        "      super(LSTM_Model, self).__init__()\n",
        "      self.num_layers = 1\n",
        "      self.batch_size = batch_size\n",
        "      self.hidden_dim = hidden_dim\n",
        "      \n",
        "      self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "      # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "      # with dimensionality hidden_dim.\n",
        "      self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers) \n",
        "      self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "      self.hidden = self.init_hidden()      \n",
        "      \n",
        "    def forward(self, sentence):\n",
        "      \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
        "        # [sent_len, batch_size, emb_dim] --> [seq_len, batch, num_directions*hidden_size]\n",
        "        preds = self.fc(lstm_out[-1].squeeze(0))\n",
        "        # [batch, num_directions*hidden_size] --> [batch_size, 1]\n",
        "        return preds\n",
        "      \n",
        "      \n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8ND7h-y9Neb",
        "colab_type": "code",
        "outputId": "40dacb0f-5345-4c12-c33f-16e37018e5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "model = LSTM_Model(vocab_size=len(TEXT.vocab), embedding_dim=300, hidden_dim=128, batch_size=BATCH_SIZE)\n",
        "model.to(device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_Model(\n",
              "  (word_embeddings): Embedding(15002, 300)\n",
              "  (lstm): LSTM(300, 128)\n",
              "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "s6lZpCaw9Nk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJcncDeCF8CA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {'train': train_iterator, \n",
        "                    'val': val_iterator}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "babVkHilG5XR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, batch_size=BATCH_SIZE):\n",
        "    since = time.time()\n",
        "\n",
        "    history = dict()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    skip_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                inputs, labels = data.text, data.airline_sentiment\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # we need to clear out the hidden state of the LSTM,\n",
        "                        # detaching it from its history on the last instance.\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    else:\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                corrects = (predicted == labels).float()\n",
        "                acc = corrects.sum()/len(corrects)\n",
        "                running_corrects += acc.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "            if phase+'_acc' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_acc'].append(epoch_acc)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_acc'] = [epoch_acc]\n",
        "            \n",
        "            if phase+'_loss' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_loss'].append(epoch_loss)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_loss'] = [epoch_loss]            \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fsZ4FAgwH_OH",
        "colab_type": "code",
        "outputId": "c5ae5002-6b09-485a-c0f6-2ff829411616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        }
      },
      "cell_type": "code",
      "source": [
        "model, history = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 0.8069 Acc: 0.6691\n",
            "val Loss: 0.6238 Acc: 0.7591\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 0.5523 Acc: 0.7832\n",
            "val Loss: 0.6016 Acc: 0.7788\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 0.4069 Acc: 0.8492\n",
            "val Loss: 0.5607 Acc: 0.7835\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 0.2963 Acc: 0.8956\n",
            "val Loss: 0.5776 Acc: 0.7856\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 0.1966 Acc: 0.9356\n",
            "val Loss: 0.7495 Acc: 0.7549\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 0.1396 Acc: 0.9558\n",
            "val Loss: 0.7743 Acc: 0.7856\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 0.0918 Acc: 0.9721\n",
            "val Loss: 0.8667 Acc: 0.7819\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 0.0658 Acc: 0.9809\n",
            "val Loss: 0.8650 Acc: 0.7882\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 0.0605 Acc: 0.9816\n",
            "val Loss: 0.9310 Acc: 0.7835\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 0.0474 Acc: 0.9863\n",
            "val Loss: 1.0955 Acc: 0.7575\n",
            "Training complete in 0m 29s\n",
            "Best val Acc: 0.788157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vc9qVM8OID3_",
        "colab_type": "code",
        "outputId": "ce2a642d-b4be-4140-84ca-82b4dd6fb0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history['train_loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNpYUjDHGtPl/hiWYcNPyRJoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkgAQQogAJQEghBABSgJACCEC1P8D7H95zeqvsa0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Zcd66NyVIXZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YomNAD_Ve3dx"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom Embedding Layer using Bidirectional LSTM"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SvC4OOKge3ee",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "class BiLSTM_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
        "      \n",
        "      super(BiLSTM_Model, self).__init__()\n",
        "      self.num_layers = 1\n",
        "      self.batch_size = batch_size\n",
        "      self.hidden_dim = hidden_dim\n",
        "      \n",
        "      self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "      # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "      # with dimensionality hidden_dim.\n",
        "      self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers, bidirectional=True) \n",
        "      self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
        "      self.hidden = self.init_hidden()      \n",
        "      \n",
        "    def forward(self, sentence):\n",
        "      \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "        bilstm_out, self.hidden = self.bilstm(embeds, self.hidden) \n",
        "        # [sent_len, batch_size, emb_dim] --> [seq_len, batch, num_directions*hidden_size]\n",
        "        preds = self.fc(bilstm_out[-1])\n",
        "        # [batch, num_directions*hidden_size] --> [batch_size, 1]\n",
        "        return preds\n",
        "      \n",
        "      \n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # The axes semantics are (num_layers*2, minibatch_size, hidden_dim)\n",
        "        return (torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim).to(device))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "69b0b098-cda3-47f9-e91e-218d4abad754",
        "id": "ZSlP0-Pfe3eh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "model = BiLSTM_Model(vocab_size=len(TEXT.vocab), embedding_dim=300, hidden_dim=128, batch_size=BATCH_SIZE)\n",
        "model.to(device)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_Model(\n",
              "  (word_embeddings): Embedding(15002, 300)\n",
              "  (bilstm): LSTM(300, 128, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UZDLKsOte3el",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1fGIOutGe3en",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {'train': train_iterator, \n",
        "                    'val': val_iterator}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OSPorkKge3er",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, batch_size=BATCH_SIZE):\n",
        "    since = time.time()\n",
        "\n",
        "    history = dict()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    skip_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                inputs, labels = data.text, data.airline_sentiment\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # we need to clear out the hidden state of the LSTM,\n",
        "                        # detaching it from its history on the last instance.\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    else:\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                corrects = (predicted == labels).float()\n",
        "                acc = corrects.sum()/len(corrects)\n",
        "                running_corrects += acc.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "            if phase+'_acc' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_acc'].append(epoch_acc)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_acc'] = [epoch_acc]\n",
        "            \n",
        "            if phase+'_loss' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_loss'].append(epoch_loss)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_loss'] = [epoch_loss]            \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "12027568-c319-477d-8289-35b25e359b12",
        "id": "DYZKLbSHe3es",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        }
      },
      "cell_type": "code",
      "source": [
        "model, history = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 0.8115 Acc: 0.6642\n",
            "val Loss: 0.6023 Acc: 0.7560\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 0.5474 Acc: 0.7838\n",
            "val Loss: 0.5629 Acc: 0.7731\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 0.3957 Acc: 0.8528\n",
            "val Loss: 0.5931 Acc: 0.7825\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 0.2803 Acc: 0.9036\n",
            "val Loss: 0.6323 Acc: 0.7721\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 0.1922 Acc: 0.9389\n",
            "val Loss: 0.6952 Acc: 0.7669\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 0.1352 Acc: 0.9584\n",
            "val Loss: 0.7886 Acc: 0.7580\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 0.0938 Acc: 0.9732\n",
            "val Loss: 0.9022 Acc: 0.7685\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 0.0642 Acc: 0.9826\n",
            "val Loss: 1.0732 Acc: 0.7460\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 0.0562 Acc: 0.9849\n",
            "val Loss: 0.9724 Acc: 0.7913\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 0.0474 Acc: 0.9866\n",
            "val Loss: 1.0127 Acc: 0.7835\n",
            "Training complete in 0m 35s\n",
            "Best val Acc: 0.791327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "7154ca46-2dd0-47e0-daf9-80bb578ab9f4",
        "id": "6EjJEQ8se3ez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history['train_loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OG7Hv11oe3e1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "s0ooeOpjfMD0"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom Embedding Layer using GRU"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n1LtxxSqfMEL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "class GRU_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
        "      \n",
        "      super(GRU_Model, self).__init__()\n",
        "      self.num_layers = 1\n",
        "      self.batch_size = batch_size\n",
        "      self.hidden_dim = hidden_dim\n",
        "      \n",
        "      self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "      # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "      # with dimensionality hidden_dim.\n",
        "      self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=self.num_layers)\n",
        "      self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "      self.hidden = self.init_hidden()      \n",
        "      \n",
        "    def forward(self, sentence):\n",
        "      \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "        gru_out, self.hidden = self.gru(embeds, self.hidden) \n",
        "        # [sent_len, batch_size, emb_dim] --> [seq_len, batch, num_directions*hidden_size]\n",
        "        preds = self.fc(gru_out[-1].squeeze(0))\n",
        "        # [batch, num_directions*hidden_size] --> [batch_size, 1]\n",
        "        return preds\n",
        "      \n",
        "      \n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "66637ef6-5015-4d8c-801c-d522cf9d0c5a",
        "id": "2FNjRLiLfMEN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "model = GRU_Model(vocab_size=len(TEXT.vocab), embedding_dim=300, hidden_dim=128, batch_size=BATCH_SIZE)\n",
        "model.to(device)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU_Model(\n",
              "  (word_embeddings): Embedding(15002, 300)\n",
              "  (gru): GRU(300, 128)\n",
              "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Elg_XD8zfMEX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "T26yqAlLfMEZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {'train': train_iterator, \n",
        "                    'val': val_iterator}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "by86t5uSfMEb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, batch_size=BATCH_SIZE):\n",
        "    since = time.time()\n",
        "\n",
        "    history = dict()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    skip_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                inputs, labels = data.text, data.airline_sentiment\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # we need to clear out the hidden state of the LSTM,\n",
        "                        # detaching it from its history on the last instance.\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    else:\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                corrects = (predicted == labels).float()\n",
        "                acc = corrects.sum()/len(corrects)\n",
        "                running_corrects += acc.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "            if phase+'_acc' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_acc'].append(epoch_acc)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_acc'] = [epoch_acc]\n",
        "            \n",
        "            if phase+'_loss' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_loss'].append(epoch_loss)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_loss'] = [epoch_loss]            \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "56c3194a-d820-43fa-c3b5-600aa658e5e7",
        "id": "8JevsEClfMEd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        }
      },
      "cell_type": "code",
      "source": [
        "model, history = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 0.7556 Acc: 0.6813\n",
            "val Loss: 0.6327 Acc: 0.7460\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 0.4965 Acc: 0.8057\n",
            "val Loss: 0.5461 Acc: 0.7737\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 0.3435 Acc: 0.8724\n",
            "val Loss: 0.5674 Acc: 0.7835\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 0.2328 Acc: 0.9154\n",
            "val Loss: 0.6658 Acc: 0.7939\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 0.1415 Acc: 0.9535\n",
            "val Loss: 0.7722 Acc: 0.7835\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 0.0893 Acc: 0.9724\n",
            "val Loss: 0.8849 Acc: 0.7669\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 0.0618 Acc: 0.9805\n",
            "val Loss: 0.9976 Acc: 0.7549\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 0.0434 Acc: 0.9875\n",
            "val Loss: 1.0308 Acc: 0.7653\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 0.0351 Acc: 0.9894\n",
            "val Loss: 1.1115 Acc: 0.7617\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 0.0314 Acc: 0.9905\n",
            "val Loss: 1.1286 Acc: 0.7757\n",
            "Training complete in 0m 27s\n",
            "Best val Acc: 0.793931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "1c3f1bb6-cc01-4a97-8b99-7538d576e2b7",
        "id": "UIHjEoGqfMEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history['train_loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE/b/A7eKdDbJA9hmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAPX/AUbTu6Q1wK8KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2F9NlsTifMEk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9f2QReJ8GZv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ]
    },
    {
      "metadata": {
        "id": "an731w9TBZc0",
        "colab_type": "code",
        "outputId": "b7c5abdd-72c3-4783-868e-7b4ce0eb516f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/huggingface/pytorch-pretrained-BERT.git"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-pretrained-BERT'...\n",
            "remote: Enumerating objects: 2550, done.\u001b[K\n",
            "remote: Total 2550 (delta 0), reused 0 (delta 0), pack-reused 2550\u001b[K\n",
            "Receiving objects: 100% (2550/2550), 1.22 MiB | 9.15 MiB/s, done.\n",
            "Resolving deltas: 100% (1724/1724), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4Bl2-LF_HpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "f51e161a-2f38-4951-ed7b-abf1528d0ec0"
      },
      "cell_type": "code",
      "source": [
        "! wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "! unzip uncased_L-12_H-768_A-12.zip\n",
        "\n",
        "! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "! unzip cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-27 20:40:06--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.70.128, 2607:f8b0:4001:c1e::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.70.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  98.2MB/s    in 4.0s    \n",
            "\n",
            "2019-03-27 20:40:10 (98.2 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "--2019-03-27 20:40:19--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 2607:f8b0:4001:c1e::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M   111MB/s    in 3.5s    \n",
            "\n",
            "2019-03-27 20:40:22 (111 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NePV1fXQXBPh",
        "colab_type": "code",
        "outputId": "a9a8449b-3eb3-4888-8bad-249106421970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XPtCH__J6FTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pytorch_pretrained_bert\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas import DataFrame\n",
        " \n",
        "le = LabelEncoder()\n",
        " \n",
        "df = pd.read_csv('https://query.data.world/s/hus7zihvuo5vt65cnv4fcfn2ppfj6y', encoding = \"ISO-8859-1\")\n",
        "\n",
        "# Creating train and dev dataframes according to BERT\n",
        "df_bert = pd.DataFrame({'user_id': df['_unit_id'],\n",
        "            'label': le.fit_transform(df['airline_sentiment']),\n",
        "            'alpha': ['a'] * df.shape[0],\n",
        "            'text': df['text'].replace(r'\\n',' ',regex=True)})\n",
        "\n",
        "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
        "  \n",
        "cols = ['user_id', 'label', 'alpha', 'text']\n",
        "     \n",
        "# # Creating test dataframe according to BERT\n",
        "# df_test = pd.read_csv(\"data/test.csv\")\n",
        "# df_bert_test = pd.DataFrame({'User_ID': df_test['_unit_id'],\n",
        "#                  'text': df_test['text']})\n",
        " \n",
        "# Saving dataframes to .tsv format as required by BERT\n",
        "df_bert_train[cols].to_csv('data/train.tsv', sep='\\t', index=False, header=False)\n",
        "df_bert_dev[cols].to_csv('data/dev.tsv', sep='\\t', index=False, header=False)\n",
        "df_bert_dev[cols].to_csv('data/test.tsv', sep='\\t', index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TTkkmkK8BipL",
        "colab_type": "code",
        "outputId": "e323973d-6dd0-4c5e-89db-a1a2185ec058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-pretrained-BERT/examples/"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch-pretrained-BERT/examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZfpGLDdSTZbq",
        "colab_type": "code",
        "outputId": "52753226-ca0e-43cb-cbea-d7ac8aff0652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile bert_classifier.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"BERT finetuning runner.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class MrpcProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            text_b = line[4]\n",
        "            label = line[0]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class MnliProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
        "            \"dev_matched\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, line[0])\n",
        "            text_a = line[8]\n",
        "            text_b = line[9]\n",
        "            label = line[-1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class ColaProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\", \"2\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            label = line[1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "class Sst2Processor(DataProcessor):\n",
        "    \"\"\"Processor for the SST-2 data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[0]\n",
        "            label = line[1]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[example.label]\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    return np.sum(outputs == labels)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--data_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
        "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
        "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
        "    parser.add_argument(\"--task_name\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The name of the task to train.\")\n",
        "    parser.add_argument(\"--output_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--cache_dir\",\n",
        "                        default=\"\",\n",
        "                        type=str,\n",
        "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
        "    parser.add_argument(\"--max_seq_length\",\n",
        "                        default=128,\n",
        "                        type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                             \"than this will be padded.\")\n",
        "    parser.add_argument(\"--do_train\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\"--do_lower_case\",\n",
        "                        action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "    parser.add_argument(\"--train_batch_size\",\n",
        "                        default=32,\n",
        "                        type=int,\n",
        "                        help=\"Total batch size for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\",\n",
        "                        default=8,\n",
        "                        type=int,\n",
        "                        help=\"Total batch size for eval.\")\n",
        "    parser.add_argument(\"--learning_rate\",\n",
        "                        default=5e-5,\n",
        "                        type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--num_train_epochs\",\n",
        "                        default=3.0,\n",
        "                        type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--warmup_proportion\",\n",
        "                        default=0.1,\n",
        "                        type=float,\n",
        "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "                             \"E.g., 0.1 = 10%% of training.\")\n",
        "    parser.add_argument(\"--no_cuda\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether not to use CUDA when available\")\n",
        "    parser.add_argument(\"--local_rank\",\n",
        "                        type=int,\n",
        "                        default=-1,\n",
        "                        help=\"local_rank for distributed training on gpus\")\n",
        "    parser.add_argument('--seed',\n",
        "                        type=int,\n",
        "                        default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--gradient_accumulation_steps',\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument('--fp16',\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "    parser.add_argument('--loss_scale',\n",
        "                        type=float, default=0,\n",
        "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
        "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    processors = {\n",
        "        \"cola\": ColaProcessor,\n",
        "        \"mnli\": MnliProcessor,\n",
        "        \"mrpc\": MrpcProcessor,\n",
        "        \"sst-2\": Sst2Processor,\n",
        "    }\n",
        "\n",
        "    num_labels_task = {\n",
        "        \"cola\": 3,\n",
        "        \"sst-2\": 2,\n",
        "        \"mnli\": 3,\n",
        "        \"mrpc\": 2,\n",
        "    }\n",
        "\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
        "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
        "\n",
        "    if args.gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                            args.gradient_accumulation_steps))\n",
        "\n",
        "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    if not args.do_train and not args.do_eval:\n",
        "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    task_name = args.task_name.lower()\n",
        "\n",
        "    if task_name not in processors:\n",
        "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "    processor = processors[task_name]()\n",
        "    num_labels = num_labels_task[task_name]\n",
        "    label_list = processor.get_labels()\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
        "\n",
        "    train_examples = None\n",
        "    num_train_optimization_steps = None\n",
        "    if args.do_train:\n",
        "        train_examples = processor.get_train_examples(args.data_dir)\n",
        "        num_train_optimization_steps = int(\n",
        "            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
        "        if args.local_rank != -1:\n",
        "            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
        "\n",
        "    # Prepare model\n",
        "    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
        "    model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
        "              cache_dir=cache_dir,\n",
        "              num_labels = num_labels)\n",
        "    if args.fp16:\n",
        "        model.half()\n",
        "    model.to(device)\n",
        "    if args.local_rank != -1:\n",
        "        try:\n",
        "            from apex.parallel import DistributedDataParallel as DDP\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "        model = DDP(model)\n",
        "    elif n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Prepare optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex.optimizers import FP16_Optimizer\n",
        "            from apex.optimizers import FusedAdam\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
        "                              lr=args.learning_rate,\n",
        "                              bias_correction=False,\n",
        "                              max_grad_norm=1.0)\n",
        "        if args.loss_scale == 0:\n",
        "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
        "        else:\n",
        "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
        "\n",
        "    else:\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                             lr=args.learning_rate,\n",
        "                             warmup=args.warmup_proportion,\n",
        "                             t_total=num_train_optimization_steps)\n",
        "\n",
        "    global_step = 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_loss = 0\n",
        "    if args.do_train:\n",
        "        train_features = convert_examples_to_features(\n",
        "            train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "        if args.local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "        model.train()\n",
        "        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, label_ids = batch\n",
        "                loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "                if n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "                if args.fp16:\n",
        "                    optimizer.backward(loss)\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    if args.fp16:\n",
        "                        # modify learning rate with special warm up BERT uses\n",
        "                        # if args.fp16 is False, BertAdam is used that handles this automatically\n",
        "                        lr_this_step = args.learning_rate * warmup_linear(global_step/num_train_optimization_steps, args.warmup_proportion)\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr_this_step\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "        # Save a trained model and the associated configuration\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n",
        "        with open(output_config_file, 'w') as f:\n",
        "            f.write(model_to_save.config.to_json_string())\n",
        "\n",
        "        # Load a trained model and config that you have fine-tuned\n",
        "        config = BertConfig(output_config_file)\n",
        "        model = BertForSequenceClassification(config, num_labels=num_labels)\n",
        "        model.load_state_dict(torch.load(output_model_file))\n",
        "    else:\n",
        "        model = BertForSequenceClassification.from_pretrained(args.bert_model, num_labels=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        eval_examples = processor.get_dev_examples(args.data_dir)\n",
        "        eval_features = convert_examples_to_features(\n",
        "            eval_examples, label_list, args.max_seq_length, tokenizer)\n",
        "        logger.info(\"***** Running evaluation *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "        # Run prediction for full data\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            input_mask = input_mask.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "                logits = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = label_ids.to('cpu').numpy()\n",
        "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            nb_eval_examples += input_ids.size(0)\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        eval_accuracy = eval_accuracy / nb_eval_examples\n",
        "        loss = tr_loss/nb_tr_steps if args.do_train else None\n",
        "        result = {'eval_loss': eval_loss,\n",
        "                  'eval_accuracy': eval_accuracy,\n",
        "                  'global_step': global_step,\n",
        "                  'loss': loss}\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing bert_classifier.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "67f27437-8c2f-4105-e53d-ac098bd1e847",
        "id": "u20LdIb1Uz3j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26301
        }
      },
      "cell_type": "code",
      "source": [
        "!python bert_classifier.py \\\n",
        "  --task_name cola \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --data_dir /content/data \\\n",
        "  --bert_model bert-base-uncased \\\n",
        "  --max_seq_length 100 \\\n",
        "  --train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --output_dir /content/output/"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "03/27/2019 20:51:08 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/27/2019 20:51:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/27/2019 20:51:09 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "03/27/2019 20:51:09 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp2k3s1kty\n",
            "03/27/2019 20:51:14 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/27/2019 20:51:19 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "03/27/2019 20:51:19 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   guid: train-0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   tokens: [CLS] @ americana ##ir iv ##e literally been holding for more than 2 hours now . i was told 2 ##hr ##s . you all are sa ##bot ##aging any chance i have of getting home [SEP]\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_ids: 101 1030 25988 4313 4921 2063 6719 2042 3173 2005 2062 2084 1016 2847 2085 1012 1045 2001 2409 1016 8093 2015 1012 2017 2035 2024 7842 18384 16594 2151 3382 1045 2031 1997 2893 2188 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   guid: train-1\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   tokens: [CLS] @ usa ##ir ##ways and despite tail ##wind ##u _ will still be late flight . [SEP]\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_ids: 101 1030 3915 4313 14035 1998 2750 5725 11101 2226 1035 2097 2145 2022 2397 3462 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   guid: train-2\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   tokens: [CLS] @ jet ##bl ##ue three hours on s delayed flight staring a blank screen . at least a heads up so you could download a movie [SEP]\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_ids: 101 1030 6892 16558 5657 2093 2847 2006 1055 8394 3462 4582 1037 8744 3898 1012 2012 2560 1037 4641 2039 2061 2017 2071 8816 1037 3185 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   guid: train-3\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   tokens: [CLS] @ southwest ##air the wait for the bags is longer than the actual flight . [SEP]\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_ids: 101 1030 4943 11215 1996 3524 2005 1996 8641 2003 2936 2084 1996 5025 3462 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   guid: train-4\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   tokens: [CLS] @ jet ##bl ##ue - hopefully that will help someone in the future . would there have been a different procedure if i had discovered the theft sooner ? [SEP]\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_ids: 101 1030 6892 16558 5657 1011 11504 2008 2097 2393 2619 1999 1996 2925 1012 2052 2045 2031 2042 1037 2367 7709 2065 1045 2018 3603 1996 11933 10076 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 20:51:21 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 20:51:28 - INFO - __main__ -   ***** Running training *****\n",
            "03/27/2019 20:51:28 - INFO - __main__ -     Num examples = 14493\n",
            "03/27/2019 20:51:28 - INFO - __main__ -     Batch size = 32\n",
            "03/27/2019 20:51:28 - INFO - __main__ -     Num steps = 1356\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/453 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/453 [00:01<09:56,  1.32s/it]\u001b[A\n",
            "Iteration:   0% 2/453 [00:02<09:40,  1.29s/it]\u001b[A\n",
            "Iteration:   1% 3/453 [00:03<09:21,  1.25s/it]\u001b[A\n",
            "Iteration:   1% 4/453 [00:04<09:07,  1.22s/it]\u001b[A\n",
            "Iteration:   1% 5/453 [00:05<08:57,  1.20s/it]\u001b[A\n",
            "Iteration:   1% 6/453 [00:07<08:50,  1.19s/it]\u001b[A\n",
            "Iteration:   2% 7/453 [00:08<08:44,  1.18s/it]\u001b[A\n",
            "Iteration:   2% 8/453 [00:09<08:40,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 9/453 [00:10<08:38,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 10/453 [00:11<08:35,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 11/453 [00:12<08:35,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 12/453 [00:14<08:33,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 13/453 [00:15<08:32,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 14/453 [00:16<08:31,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 15/453 [00:17<08:30,  1.17s/it]\u001b[A\n",
            "Iteration:   4% 16/453 [00:18<08:27,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 17/453 [00:19<08:25,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 18/453 [00:21<08:23,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 19/453 [00:22<08:22,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 20/453 [00:23<08:20,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 21/453 [00:24<08:18,  1.15s/it]\u001b[A\n",
            "Iteration:   5% 22/453 [00:25<08:17,  1.15s/it]\u001b[A\n",
            "Iteration:   5% 23/453 [00:26<08:16,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 24/453 [00:27<08:16,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 25/453 [00:29<08:16,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 26/453 [00:30<08:16,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 27/453 [00:31<08:14,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 28/453 [00:32<08:12,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 29/453 [00:33<08:12,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 30/453 [00:34<08:10,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 31/453 [00:36<08:09,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 32/453 [00:37<08:08,  1.16s/it]\u001b[A\n",
            "Iteration:   7% 33/453 [00:38<08:08,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 34/453 [00:39<08:07,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 35/453 [00:40<08:06,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 36/453 [00:41<08:04,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 37/453 [00:43<08:03,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 38/453 [00:44<08:01,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 39/453 [00:45<08:01,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 40/453 [00:46<08:00,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 41/453 [00:47<07:59,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 42/453 [00:48<07:59,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 43/453 [00:50<07:56,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 44/453 [00:51<07:54,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 45/453 [00:52<07:53,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 46/453 [00:53<07:53,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 47/453 [00:54<07:51,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 48/453 [00:55<07:49,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 49/453 [00:57<07:48,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 50/453 [00:58<07:47,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 51/453 [00:59<07:46,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 52/453 [01:00<07:45,  1.16s/it]\u001b[A\n",
            "Iteration:  12% 53/453 [01:01<07:45,  1.16s/it]\u001b[A\n",
            "Iteration:  12% 54/453 [01:02<07:43,  1.16s/it]\u001b[A\n",
            "Iteration:  12% 55/453 [01:04<07:42,  1.16s/it]\u001b[A\n",
            "Iteration:  12% 56/453 [01:05<07:41,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 57/453 [01:06<07:40,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 58/453 [01:07<07:39,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 59/453 [01:08<07:38,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 60/453 [01:09<07:37,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 61/453 [01:10<07:35,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 62/453 [01:12<07:34,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 63/453 [01:13<07:34,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 64/453 [01:14<07:32,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 65/453 [01:15<07:31,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 66/453 [01:16<07:29,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 67/453 [01:17<07:29,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 68/453 [01:19<07:28,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 69/453 [01:20<07:26,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 70/453 [01:21<07:26,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 71/453 [01:22<07:26,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 72/453 [01:23<07:24,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 73/453 [01:24<07:23,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 74/453 [01:26<07:22,  1.17s/it]\u001b[A\n",
            "Iteration:  17% 75/453 [01:27<07:20,  1.17s/it]\u001b[A\n",
            "Iteration:  17% 76/453 [01:28<07:19,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 77/453 [01:29<07:18,  1.17s/it]\u001b[A\n",
            "Iteration:  17% 78/453 [01:30<07:16,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 79/453 [01:31<07:15,  1.16s/it]\u001b[A\n",
            "Iteration:  18% 80/453 [01:33<07:14,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 81/453 [01:34<07:13,  1.16s/it]\u001b[A\n",
            "Iteration:  18% 82/453 [01:35<07:11,  1.16s/it]\u001b[A\n",
            "Iteration:  18% 83/453 [01:36<07:10,  1.16s/it]\u001b[A\n",
            "Iteration:  19% 84/453 [01:37<07:09,  1.16s/it]\u001b[A\n",
            "Iteration:  19% 85/453 [01:38<07:08,  1.16s/it]\u001b[A\n",
            "Iteration:  19% 86/453 [01:40<07:07,  1.16s/it]\u001b[A\n",
            "Iteration:  19% 87/453 [01:41<07:06,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 88/453 [01:42<07:05,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 89/453 [01:43<07:03,  1.16s/it]\u001b[A\n",
            "Iteration:  20% 90/453 [01:44<07:02,  1.16s/it]\u001b[A\n",
            "Iteration:  20% 91/453 [01:45<07:01,  1.16s/it]\u001b[A\n",
            "Iteration:  20% 92/453 [01:47<07:00,  1.16s/it]\u001b[A\n",
            "Iteration:  21% 93/453 [01:48<06:58,  1.16s/it]\u001b[A\n",
            "Iteration:  21% 94/453 [01:49<06:58,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 95/453 [01:50<06:56,  1.16s/it]\u001b[A\n",
            "Iteration:  21% 96/453 [01:51<06:55,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 97/453 [01:52<06:54,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 98/453 [01:54<06:52,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 99/453 [01:55<06:51,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 100/453 [01:56<06:50,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 101/453 [01:57<06:49,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 102/453 [01:58<06:47,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 103/453 [01:59<06:48,  1.17s/it]\u001b[A\n",
            "Iteration:  23% 104/453 [02:01<06:45,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 105/453 [02:02<06:44,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 106/453 [02:03<06:43,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 107/453 [02:04<06:42,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 108/453 [02:05<06:41,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 109/453 [02:06<06:40,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 110/453 [02:08<06:39,  1.16s/it]\u001b[A\n",
            "Iteration:  25% 111/453 [02:09<06:39,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 112/453 [02:10<06:37,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 113/453 [02:11<06:35,  1.16s/it]\u001b[A\n",
            "Iteration:  25% 114/453 [02:12<06:34,  1.16s/it]\u001b[A\n",
            "Iteration:  25% 115/453 [02:13<06:33,  1.16s/it]\u001b[A\n",
            "Iteration:  26% 116/453 [02:15<06:32,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 117/453 [02:16<06:32,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 118/453 [02:17<06:30,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 119/453 [02:18<06:29,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 120/453 [02:19<06:28,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 121/453 [02:20<06:26,  1.16s/it]\u001b[A\n",
            "Iteration:  27% 122/453 [02:22<06:27,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 123/453 [02:23<06:26,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 124/453 [02:24<06:23,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 125/453 [02:25<06:21,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 126/453 [02:26<06:20,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 127/453 [02:27<06:20,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 128/453 [02:29<06:19,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 129/453 [02:30<06:21,  1.18s/it]\u001b[A\n",
            "Iteration:  29% 130/453 [02:31<06:19,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 131/453 [02:32<06:17,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 132/453 [02:33<06:15,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 133/453 [02:34<06:14,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 134/453 [02:36<06:13,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 135/453 [02:37<06:11,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 136/453 [02:38<06:10,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 137/453 [02:39<06:09,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 138/453 [02:40<06:08,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 139/453 [02:41<06:09,  1.18s/it]\u001b[A\n",
            "Iteration:  31% 140/453 [02:43<06:06,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 141/453 [02:44<06:05,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 142/453 [02:45<06:04,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 143/453 [02:46<06:01,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 144/453 [02:47<06:00,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 145/453 [02:48<05:59,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 146/453 [02:50<05:58,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 147/453 [02:51<05:57,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 148/453 [02:52<05:56,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 149/453 [02:53<05:55,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 150/453 [02:54<05:53,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 151/453 [02:55<05:52,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 152/453 [02:57<05:51,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 153/453 [02:58<05:50,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 154/453 [02:59<05:49,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 155/453 [03:00<05:47,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 156/453 [03:01<05:46,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 157/453 [03:02<05:45,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 158/453 [03:04<05:44,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 159/453 [03:05<05:42,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 160/453 [03:06<05:41,  1.16s/it]\u001b[A\n",
            "Iteration:  36% 161/453 [03:07<05:40,  1.16s/it]\u001b[A\n",
            "Iteration:  36% 162/453 [03:08<05:38,  1.16s/it]\u001b[A\n",
            "Iteration:  36% 163/453 [03:09<05:37,  1.16s/it]\u001b[A\n",
            "Iteration:  36% 164/453 [03:11<05:35,  1.16s/it]\u001b[A\n",
            "Iteration:  36% 165/453 [03:12<05:35,  1.16s/it]\u001b[A\n",
            "Iteration:  37% 166/453 [03:13<05:33,  1.16s/it]\u001b[A\n",
            "Iteration:  37% 167/453 [03:14<05:34,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 168/453 [03:15<05:32,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 169/453 [03:16<05:30,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 170/453 [03:18<05:30,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 171/453 [03:19<05:29,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 172/453 [03:20<05:27,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 173/453 [03:21<05:26,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 174/453 [03:22<05:25,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 175/453 [03:23<05:26,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 176/453 [03:25<05:24,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 177/453 [03:26<05:23,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 178/453 [03:27<05:21,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 179/453 [03:28<05:19,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 180/453 [03:29<05:20,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 181/453 [03:30<05:18,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 182/453 [03:32<05:16,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 183/453 [03:33<05:15,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 184/453 [03:34<05:14,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 185/453 [03:35<05:13,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 186/453 [03:36<05:12,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 187/453 [03:37<05:10,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 188/453 [03:39<05:09,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 189/453 [03:40<05:08,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 190/453 [03:41<05:07,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 191/453 [03:42<05:06,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 192/453 [03:43<05:04,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 193/453 [03:45<05:03,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 194/453 [03:46<05:02,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 195/453 [03:47<05:01,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 196/453 [03:48<04:59,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 197/453 [03:49<04:58,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 198/453 [03:50<04:57,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 199/453 [03:52<04:56,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 200/453 [03:53<04:55,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 201/453 [03:54<04:53,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 202/453 [03:55<04:53,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 203/453 [03:56<04:52,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 204/453 [03:57<04:50,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 205/453 [03:59<04:51,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 206/453 [04:00<04:50,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 207/453 [04:01<04:47,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 208/453 [04:02<04:47,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 209/453 [04:03<04:45,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 210/453 [04:04<04:44,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 211/453 [04:06<04:43,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 212/453 [04:07<04:41,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 213/453 [04:08<04:40,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 214/453 [04:09<04:39,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 215/453 [04:10<04:38,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 216/453 [04:11<04:36,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 217/453 [04:13<04:35,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 218/453 [04:14<04:34,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 219/453 [04:15<04:33,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 220/453 [04:16<04:32,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 221/453 [04:17<04:30,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 222/453 [04:18<04:29,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 223/453 [04:20<04:28,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 224/453 [04:21<04:26,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 225/453 [04:22<04:25,  1.16s/it]\u001b[A\n",
            "Iteration:  50% 226/453 [04:23<04:24,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 227/453 [04:24<04:22,  1.16s/it]\u001b[A\n",
            "Iteration:  50% 228/453 [04:25<04:21,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 229/453 [04:27<04:20,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 230/453 [04:28<04:19,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 231/453 [04:29<04:17,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 232/453 [04:30<04:16,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 233/453 [04:31<04:15,  1.16s/it]\u001b[A\n",
            "Iteration:  52% 234/453 [04:32<04:14,  1.16s/it]\u001b[A\n",
            "Iteration:  52% 235/453 [04:33<04:13,  1.16s/it]\u001b[A\n",
            "Iteration:  52% 236/453 [04:35<04:12,  1.16s/it]\u001b[A\n",
            "Iteration:  52% 237/453 [04:36<04:11,  1.16s/it]\u001b[A\n",
            "Iteration:  53% 238/453 [04:37<04:10,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 239/453 [04:38<04:09,  1.16s/it]\u001b[A\n",
            "Iteration:  53% 240/453 [04:39<04:08,  1.16s/it]\u001b[A\n",
            "Iteration:  53% 241/453 [04:40<04:06,  1.16s/it]\u001b[A\n",
            "Iteration:  53% 242/453 [04:42<04:05,  1.16s/it]\u001b[A\n",
            "Iteration:  54% 243/453 [04:43<04:03,  1.16s/it]\u001b[A\n",
            "Iteration:  54% 244/453 [04:44<04:02,  1.16s/it]\u001b[A\n",
            "Iteration:  54% 245/453 [04:45<04:01,  1.16s/it]\u001b[A\n",
            "Iteration:  54% 246/453 [04:46<03:59,  1.16s/it]\u001b[A\n",
            "Iteration:  55% 247/453 [04:47<03:58,  1.16s/it]\u001b[A\n",
            "Iteration:  55% 248/453 [04:49<03:57,  1.16s/it]\u001b[A\n",
            "Iteration:  55% 249/453 [04:50<03:57,  1.16s/it]\u001b[A\n",
            "Iteration:  55% 250/453 [04:51<03:56,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 251/453 [04:52<03:55,  1.16s/it]\u001b[A\n",
            "Iteration:  56% 252/453 [04:53<03:53,  1.16s/it]\u001b[A\n",
            "Iteration:  56% 253/453 [04:54<03:52,  1.16s/it]\u001b[A\n",
            "Iteration:  56% 254/453 [04:56<03:52,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 255/453 [04:57<03:51,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 256/453 [04:58<03:49,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 257/453 [04:59<03:48,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 258/453 [05:00<03:47,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 259/453 [05:01<03:45,  1.16s/it]\u001b[A\n",
            "Iteration:  57% 260/453 [05:03<03:44,  1.16s/it]\u001b[A\n",
            "Iteration:  58% 261/453 [05:04<03:43,  1.16s/it]\u001b[A\n",
            "Iteration:  58% 262/453 [05:05<03:42,  1.16s/it]\u001b[A\n",
            "Iteration:  58% 263/453 [05:06<03:41,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 264/453 [05:07<03:40,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 265/453 [05:08<03:39,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 266/453 [05:10<03:37,  1.16s/it]\u001b[A\n",
            "Iteration:  59% 267/453 [05:11<03:36,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 268/453 [05:12<03:35,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 269/453 [05:13<03:34,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 270/453 [05:14<03:33,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 271/453 [05:15<03:32,  1.16s/it]\u001b[A\n",
            "Iteration:  60% 272/453 [05:17<03:31,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 273/453 [05:18<03:29,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 274/453 [05:19<03:28,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 275/453 [05:20<03:27,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 276/453 [05:21<03:26,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 277/453 [05:22<03:25,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 278/453 [05:24<03:24,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 279/453 [05:25<03:22,  1.16s/it]\u001b[A\n",
            "Iteration:  62% 280/453 [05:26<03:21,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 281/453 [05:27<03:20,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 282/453 [05:28<03:19,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 283/453 [05:29<03:18,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 284/453 [05:31<03:16,  1.16s/it]\u001b[A\n",
            "Iteration:  63% 285/453 [05:32<03:15,  1.16s/it]\u001b[A\n",
            "Iteration:  63% 286/453 [05:33<03:14,  1.16s/it]\u001b[A\n",
            "Iteration:  63% 287/453 [05:34<03:13,  1.16s/it]\u001b[A\n",
            "Iteration:  64% 288/453 [05:35<03:12,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 289/453 [05:36<03:11,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 290/453 [05:38<03:09,  1.16s/it]\u001b[A\n",
            "Iteration:  64% 291/453 [05:39<03:08,  1.16s/it]\u001b[A\n",
            "Iteration:  64% 292/453 [05:40<03:07,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 293/453 [05:41<03:06,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 294/453 [05:42<03:05,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 295/453 [05:43<03:03,  1.16s/it]\u001b[A\n",
            "Iteration:  65% 296/453 [05:45<03:03,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 297/453 [05:46<03:01,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 298/453 [05:47<03:00,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 299/453 [05:48<02:59,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 300/453 [05:49<02:58,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 301/453 [05:50<02:57,  1.17s/it]\u001b[A\n",
            "Iteration:  67% 302/453 [05:52<02:56,  1.17s/it]\u001b[A\n",
            "Iteration:  67% 303/453 [05:53<02:55,  1.17s/it]\u001b[A\n",
            "Iteration:  67% 304/453 [05:54<02:54,  1.17s/it]\u001b[A\n",
            "Iteration:  67% 305/453 [05:55<02:53,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 306/453 [05:56<02:51,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 307/453 [05:57<02:50,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 308/453 [05:59<02:49,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 309/453 [06:00<02:48,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 310/453 [06:01<02:47,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 311/453 [06:02<02:46,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 312/453 [06:03<02:45,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 313/453 [06:04<02:43,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 314/453 [06:06<02:42,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 315/453 [06:07<02:40,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 316/453 [06:08<02:39,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 317/453 [06:09<02:38,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 318/453 [06:10<02:37,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 319/453 [06:11<02:36,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 320/453 [06:13<02:35,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 321/453 [06:14<02:34,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 322/453 [06:15<02:32,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 323/453 [06:16<02:31,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 324/453 [06:17<02:30,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 325/453 [06:18<02:29,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 326/453 [06:20<02:28,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 327/453 [06:21<02:26,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 328/453 [06:22<02:25,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 329/453 [06:23<02:24,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 330/453 [06:24<02:23,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 331/453 [06:25<02:22,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 332/453 [06:27<02:21,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 333/453 [06:28<02:19,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 334/453 [06:29<02:19,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 335/453 [06:30<02:17,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 336/453 [06:31<02:16,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 337/453 [06:32<02:15,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 338/453 [06:34<02:14,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 339/453 [06:35<02:13,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 340/453 [06:36<02:12,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 341/453 [06:37<02:10,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 342/453 [06:38<02:09,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 343/453 [06:39<02:08,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 344/453 [06:41<02:07,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 345/453 [06:42<02:05,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 346/453 [06:43<02:04,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 347/453 [06:44<02:03,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 348/453 [06:45<02:02,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 349/453 [06:46<02:01,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 350/453 [06:48<02:00,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 351/453 [06:49<01:59,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 352/453 [06:50<01:57,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 353/453 [06:51<01:56,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 354/453 [06:52<01:55,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 355/453 [06:53<01:54,  1.16s/it]\u001b[A\n",
            "Iteration:  79% 356/453 [06:55<01:53,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 357/453 [06:56<01:52,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 358/453 [06:57<01:50,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 359/453 [06:58<01:49,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 360/453 [06:59<01:48,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 361/453 [07:00<01:47,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 362/453 [07:02<01:46,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 363/453 [07:03<01:44,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 364/453 [07:04<01:43,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 365/453 [07:05<01:42,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 366/453 [07:06<01:41,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 367/453 [07:07<01:40,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 368/453 [07:09<01:39,  1.16s/it]\u001b[A\n",
            "Iteration:  81% 369/453 [07:10<01:37,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 370/453 [07:11<01:36,  1.16s/it]\u001b[A\n",
            "Iteration:  82% 371/453 [07:12<01:35,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 372/453 [07:13<01:34,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 373/453 [07:14<01:33,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 374/453 [07:16<01:32,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 375/453 [07:17<01:31,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 376/453 [07:18<01:29,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 377/453 [07:19<01:28,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 378/453 [07:20<01:27,  1.16s/it]\u001b[A\n",
            "Iteration:  84% 379/453 [07:21<01:26,  1.16s/it]\u001b[A\n",
            "Iteration:  84% 380/453 [07:23<01:25,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 381/453 [07:24<01:23,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 382/453 [07:25<01:22,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 383/453 [07:26<01:21,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 384/453 [07:27<01:20,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 385/453 [07:28<01:19,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 386/453 [07:30<01:18,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 387/453 [07:31<01:16,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 388/453 [07:32<01:15,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 389/453 [07:33<01:14,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 390/453 [07:34<01:13,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 391/453 [07:35<01:12,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 392/453 [07:37<01:11,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 393/453 [07:38<01:10,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 394/453 [07:39<01:08,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 395/453 [07:40<01:07,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 396/453 [07:41<01:06,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 397/453 [07:42<01:05,  1.16s/it]\u001b[A\n",
            "Iteration:  88% 398/453 [07:44<01:04,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 399/453 [07:45<01:02,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 400/453 [07:46<01:01,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 401/453 [07:47<01:00,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 402/453 [07:48<00:59,  1.16s/it]\u001b[A\n",
            "Iteration:  89% 403/453 [07:49<00:58,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 404/453 [07:51<00:57,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 405/453 [07:52<00:56,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 406/453 [07:53<00:54,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 407/453 [07:54<00:53,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 408/453 [07:55<00:52,  1.16s/it]\u001b[A\n",
            "Iteration:  90% 409/453 [07:56<00:51,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 410/453 [07:58<00:50,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 411/453 [07:59<00:49,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 412/453 [08:00<00:47,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 413/453 [08:01<00:46,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 414/453 [08:02<00:45,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 415/453 [08:03<00:44,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 416/453 [08:05<00:43,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 417/453 [08:06<00:42,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 418/453 [08:07<00:40,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 419/453 [08:08<00:39,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 420/453 [08:09<00:38,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 421/453 [08:10<00:37,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 422/453 [08:12<00:36,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 423/453 [08:13<00:34,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 424/453 [08:14<00:33,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 425/453 [08:15<00:32,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 426/453 [08:16<00:31,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 427/453 [08:17<00:30,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 428/453 [08:19<00:29,  1.16s/it]\u001b[A\n",
            "Iteration:  95% 429/453 [08:20<00:27,  1.16s/it]\u001b[A\n",
            "Iteration:  95% 430/453 [08:21<00:26,  1.16s/it]\u001b[A\n",
            "Iteration:  95% 431/453 [08:22<00:25,  1.16s/it]\u001b[A\n",
            "Iteration:  95% 432/453 [08:23<00:24,  1.16s/it]\u001b[A\n",
            "Iteration:  96% 433/453 [08:24<00:23,  1.16s/it]\u001b[A\n",
            "Iteration:  96% 434/453 [08:26<00:22,  1.16s/it]\u001b[A\n",
            "Iteration:  96% 435/453 [08:27<00:20,  1.16s/it]\u001b[A\n",
            "Iteration:  96% 436/453 [08:28<00:19,  1.16s/it]\u001b[A\n",
            "Iteration:  96% 437/453 [08:29<00:18,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 438/453 [08:30<00:17,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 439/453 [08:31<00:16,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 440/453 [08:33<00:15,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 441/453 [08:34<00:13,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 442/453 [08:35<00:12,  1.16s/it]\u001b[A\n",
            "Iteration:  98% 443/453 [08:36<00:11,  1.16s/it]\u001b[A\n",
            "Iteration:  98% 444/453 [08:37<00:10,  1.16s/it]\u001b[A\n",
            "Iteration:  98% 445/453 [08:38<00:09,  1.16s/it]\u001b[A\n",
            "Iteration:  98% 446/453 [08:40<00:08,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 447/453 [08:41<00:06,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 448/453 [08:42<00:05,  1.17s/it]\u001b[A\n",
            "Iteration:  99% 449/453 [08:43<00:04,  1.17s/it]\u001b[A\n",
            "Iteration:  99% 450/453 [08:44<00:03,  1.17s/it]\u001b[A\n",
            "Iteration: 100% 451/453 [08:45<00:02,  1.17s/it]\u001b[A\n",
            "Iteration: 100% 452/453 [08:47<00:01,  1.17s/it]\u001b[A\n",
            "Iteration: 100% 453/453 [08:48<00:00,  1.14s/it]\u001b[A\n",
            "Epoch:  33% 1/3 [08:48<17:36, 528.10s/it]\n",
            "Iteration:   0% 0/453 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/453 [00:01<08:45,  1.16s/it]\u001b[A\n",
            "Iteration:   0% 2/453 [00:02<08:44,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 3/453 [00:03<08:43,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 4/453 [00:04<08:42,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 5/453 [00:05<08:42,  1.17s/it]\u001b[A\n",
            "Iteration:   1% 6/453 [00:06<08:41,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 7/453 [00:08<08:40,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 8/453 [00:09<08:38,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 9/453 [00:10<08:38,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 10/453 [00:11<08:37,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 11/453 [00:12<08:36,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 12/453 [00:14<08:35,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 13/453 [00:15<08:33,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 14/453 [00:16<08:32,  1.17s/it]\u001b[A\n",
            "Iteration:   3% 15/453 [00:17<08:35,  1.18s/it]\u001b[A\n",
            "Iteration:   4% 16/453 [00:18<08:32,  1.17s/it]\u001b[A\n",
            "Iteration:   4% 17/453 [00:19<08:29,  1.17s/it]\u001b[A\n",
            "Iteration:   4% 18/453 [00:21<08:31,  1.18s/it]\u001b[A\n",
            "Iteration:   4% 19/453 [00:22<08:29,  1.17s/it]\u001b[A\n",
            "Iteration:   4% 20/453 [00:23<08:27,  1.17s/it]\u001b[A\n",
            "Iteration:   5% 21/453 [00:24<08:26,  1.17s/it]\u001b[A\n",
            "Iteration:   5% 22/453 [00:25<08:24,  1.17s/it]\u001b[A\n",
            "Iteration:   5% 23/453 [00:26<08:22,  1.17s/it]\u001b[A\n",
            "Iteration:   5% 24/453 [00:28<08:20,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 25/453 [00:29<08:20,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 26/453 [00:30<08:18,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 27/453 [00:31<08:18,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 28/453 [00:32<08:18,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 29/453 [00:33<08:15,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 30/453 [00:35<08:14,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 31/453 [00:36<08:13,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 32/453 [00:37<08:11,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 33/453 [00:38<08:10,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 34/453 [00:39<08:09,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 35/453 [00:40<08:07,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 36/453 [00:42<08:06,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 37/453 [00:43<08:06,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 38/453 [00:44<08:04,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 39/453 [00:45<08:03,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 40/453 [00:46<08:02,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 41/453 [00:47<08:00,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 42/453 [00:49<07:58,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 43/453 [00:50<07:57,  1.17s/it]\u001b[A\n",
            "Iteration:  10% 44/453 [00:51<07:56,  1.17s/it]\u001b[A\n",
            "Iteration:  10% 45/453 [00:52<07:56,  1.17s/it]\u001b[A\n",
            "Iteration:  10% 46/453 [00:53<07:55,  1.17s/it]\u001b[A\n",
            "Iteration:  10% 47/453 [00:54<07:54,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 48/453 [00:56<07:52,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 49/453 [00:57<07:51,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 50/453 [00:58<07:49,  1.16s/it]\u001b[A\n",
            "Iteration:  11% 51/453 [00:59<07:51,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 52/453 [01:00<07:49,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 53/453 [01:01<07:48,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 54/453 [01:03<07:46,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 55/453 [01:04<07:45,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 56/453 [01:05<07:46,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 57/453 [01:06<07:43,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 58/453 [01:07<07:42,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 59/453 [01:08<07:41,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 60/453 [01:10<07:39,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 61/453 [01:11<07:37,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 62/453 [01:12<07:35,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 63/453 [01:13<07:34,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 64/453 [01:14<07:33,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 65/453 [01:15<07:32,  1.17s/it]\u001b[A\n",
            "Iteration:  15% 66/453 [01:17<07:31,  1.17s/it]\u001b[A\n",
            "Iteration:  15% 67/453 [01:18<07:31,  1.17s/it]\u001b[A\n",
            "Iteration:  15% 68/453 [01:19<07:29,  1.17s/it]\u001b[A\n",
            "Iteration:  15% 69/453 [01:20<07:27,  1.17s/it]\u001b[A\n",
            "Iteration:  15% 70/453 [01:21<07:27,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 71/453 [01:22<07:27,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 72/453 [01:24<07:25,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 73/453 [01:25<07:23,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 74/453 [01:26<07:25,  1.18s/it]\u001b[A\n",
            "Iteration:  17% 75/453 [01:27<07:25,  1.18s/it]\u001b[A\n",
            "Iteration:  17% 76/453 [01:28<07:25,  1.18s/it]\u001b[A\n",
            "Iteration:  17% 77/453 [01:30<07:22,  1.18s/it]\u001b[A\n",
            "Iteration:  17% 78/453 [01:31<07:19,  1.17s/it]\u001b[A\n",
            "Iteration:  17% 79/453 [01:32<07:17,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 80/453 [01:33<07:16,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 81/453 [01:34<07:17,  1.18s/it]\u001b[A\n",
            "Iteration:  18% 82/453 [01:35<07:14,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 83/453 [01:37<07:13,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 84/453 [01:38<07:12,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 85/453 [01:39<07:10,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 86/453 [01:40<07:10,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 87/453 [01:41<07:08,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 88/453 [01:42<07:06,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 89/453 [01:44<07:05,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 90/453 [01:45<07:04,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 91/453 [01:46<07:02,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 92/453 [01:47<07:01,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 93/453 [01:48<07:00,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 94/453 [01:49<06:58,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 95/453 [01:51<06:57,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 96/453 [01:52<06:56,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 97/453 [01:53<06:55,  1.17s/it]\u001b[A\n",
            "Iteration:  22% 98/453 [01:54<06:53,  1.17s/it]\u001b[A\n",
            "Iteration:  22% 99/453 [01:55<06:52,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 100/453 [01:56<06:50,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 101/453 [01:58<06:49,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 102/453 [01:59<06:48,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 103/453 [02:00<06:47,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 104/453 [02:01<06:45,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 105/453 [02:02<06:44,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 106/453 [02:03<06:44,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 107/453 [02:05<06:43,  1.17s/it]\u001b[A\n",
            "Iteration:  24% 108/453 [02:06<06:41,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 109/453 [02:07<06:40,  1.16s/it]\u001b[A\n",
            "Iteration:  24% 110/453 [02:08<06:39,  1.16s/it]\u001b[A\n",
            "Iteration:  25% 111/453 [02:09<06:38,  1.16s/it]\u001b[A\n",
            "Iteration:  25% 112/453 [02:10<06:37,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 113/453 [02:12<06:36,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 114/453 [02:13<06:34,  1.16s/it]\u001b[A\n",
            "Iteration:  25% 115/453 [02:14<06:33,  1.16s/it]\u001b[A\n",
            "Iteration:  26% 116/453 [02:15<06:31,  1.16s/it]\u001b[A\n",
            "Iteration:  26% 117/453 [02:16<06:30,  1.16s/it]\u001b[A\n",
            "Iteration:  26% 118/453 [02:17<06:30,  1.16s/it]\u001b[A\n",
            "Iteration:  26% 119/453 [02:19<06:28,  1.16s/it]\u001b[A\n",
            "Iteration:  26% 120/453 [02:20<06:28,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 121/453 [02:21<06:27,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 122/453 [02:22<06:25,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 123/453 [02:23<06:23,  1.16s/it]\u001b[A\n",
            "Iteration:  27% 124/453 [02:24<06:22,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 125/453 [02:25<06:21,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 126/453 [02:27<06:20,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 127/453 [02:28<06:20,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 128/453 [02:29<06:19,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 129/453 [02:30<06:17,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 130/453 [02:31<06:16,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 131/453 [02:32<06:15,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 132/453 [02:34<06:14,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 133/453 [02:35<06:13,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 134/453 [02:36<06:12,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 135/453 [02:37<06:11,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 136/453 [02:38<06:10,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 137/453 [02:40<06:09,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 138/453 [02:41<06:08,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 139/453 [02:42<06:07,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 140/453 [02:43<06:05,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 141/453 [02:44<06:04,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 142/453 [02:45<06:02,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 143/453 [02:47<06:02,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 144/453 [02:48<06:01,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 145/453 [02:49<05:59,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 146/453 [02:50<05:58,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 147/453 [02:51<05:56,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 148/453 [02:52<05:58,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 149/453 [02:54<05:56,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 150/453 [02:55<05:54,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 151/453 [02:56<05:53,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 152/453 [02:57<05:51,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 153/453 [02:58<05:50,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 154/453 [02:59<05:49,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 155/453 [03:01<05:48,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 156/453 [03:02<05:46,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 157/453 [03:03<05:45,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 158/453 [03:04<05:43,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 159/453 [03:05<05:42,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 160/453 [03:06<05:42,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 161/453 [03:08<05:41,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 162/453 [03:09<05:40,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 163/453 [03:10<05:39,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 164/453 [03:11<05:38,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 165/453 [03:12<05:36,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 166/453 [03:13<05:36,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 167/453 [03:15<05:35,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 168/453 [03:16<05:33,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 169/453 [03:17<05:31,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 170/453 [03:18<05:30,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 171/453 [03:19<05:29,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 172/453 [03:20<05:28,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 173/453 [03:22<05:26,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 174/453 [03:23<05:25,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 175/453 [03:24<05:24,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 176/453 [03:25<05:23,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 177/453 [03:26<05:22,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 178/453 [03:27<05:20,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 179/453 [03:29<05:19,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 180/453 [03:30<05:18,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 181/453 [03:31<05:16,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 182/453 [03:32<05:16,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 183/453 [03:33<05:14,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 184/453 [03:34<05:13,  1.16s/it]\u001b[A\n",
            "Iteration:  41% 185/453 [03:36<05:12,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 186/453 [03:37<05:11,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 187/453 [03:38<05:10,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 188/453 [03:39<05:09,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 189/453 [03:40<05:09,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 190/453 [03:41<05:07,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 191/453 [03:43<05:06,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 192/453 [03:44<05:04,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 193/453 [03:45<05:04,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 194/453 [03:46<05:03,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 195/453 [03:47<05:01,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 196/453 [03:48<05:00,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 197/453 [03:50<04:58,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 198/453 [03:51<04:57,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 199/453 [03:52<04:56,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 200/453 [03:53<04:55,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 201/453 [03:54<04:54,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 202/453 [03:55<04:53,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 203/453 [03:57<04:52,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 204/453 [03:58<04:51,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 205/453 [03:59<04:50,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 206/453 [04:00<04:48,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 207/453 [04:01<04:46,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 208/453 [04:02<04:46,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 209/453 [04:04<04:44,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 210/453 [04:05<04:43,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 211/453 [04:06<04:42,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 212/453 [04:07<04:41,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 213/453 [04:08<04:40,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 214/453 [04:09<04:39,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 215/453 [04:11<04:38,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 216/453 [04:12<04:36,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 217/453 [04:13<04:35,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 218/453 [04:14<04:33,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 219/453 [04:15<04:32,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 220/453 [04:16<04:32,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 221/453 [04:18<04:31,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 222/453 [04:19<04:29,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 223/453 [04:20<04:28,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 224/453 [04:21<04:27,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 225/453 [04:22<04:25,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 226/453 [04:23<04:24,  1.16s/it]\u001b[A\n",
            "Iteration:  50% 227/453 [04:25<04:24,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 228/453 [04:26<04:22,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 229/453 [04:27<04:21,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 230/453 [04:28<04:20,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 231/453 [04:29<04:19,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 232/453 [04:30<04:17,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 233/453 [04:32<04:16,  1.17s/it]\u001b[A\n",
            "Iteration:  52% 234/453 [04:33<04:14,  1.16s/it]\u001b[A\n",
            "Iteration:  52% 235/453 [04:34<04:14,  1.17s/it]\u001b[A\n",
            "Iteration:  52% 236/453 [04:35<04:12,  1.16s/it]\u001b[A\n",
            "Iteration:  52% 237/453 [04:36<04:11,  1.16s/it]\u001b[A\n",
            "Iteration:  53% 238/453 [04:37<04:10,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 239/453 [04:39<04:09,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 240/453 [04:40<04:08,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 241/453 [04:41<04:07,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 242/453 [04:42<04:05,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 243/453 [04:43<04:05,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 244/453 [04:44<04:04,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 245/453 [04:46<04:02,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 246/453 [04:47<04:01,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 247/453 [04:48<04:00,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 248/453 [04:49<03:59,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 249/453 [04:50<03:57,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 250/453 [04:51<03:56,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 251/453 [04:53<03:55,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 252/453 [04:54<03:53,  1.16s/it]\u001b[A\n",
            "Iteration:  56% 253/453 [04:55<03:53,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 254/453 [04:56<03:52,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 255/453 [04:57<03:50,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 256/453 [04:58<03:49,  1.16s/it]\u001b[A\n",
            "Iteration:  57% 257/453 [05:00<03:48,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 258/453 [05:01<03:48,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 259/453 [05:02<03:46,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 260/453 [05:03<03:45,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 261/453 [05:04<03:44,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 262/453 [05:05<03:42,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 263/453 [05:07<03:41,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 264/453 [05:08<03:40,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 265/453 [05:09<03:39,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 266/453 [05:10<03:38,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 267/453 [05:11<03:37,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 268/453 [05:12<03:36,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 269/453 [05:14<03:35,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 270/453 [05:15<03:33,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 271/453 [05:16<03:32,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 272/453 [05:17<03:30,  1.16s/it]\u001b[A\n",
            "Iteration:  60% 273/453 [05:18<03:29,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 274/453 [05:19<03:28,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 275/453 [05:21<03:27,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 276/453 [05:22<03:26,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 277/453 [05:23<03:25,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 278/453 [05:24<03:24,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 279/453 [05:25<03:22,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 280/453 [05:26<03:21,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 281/453 [05:28<03:20,  1.16s/it]\u001b[A\n",
            "Iteration:  62% 282/453 [05:29<03:19,  1.16s/it]\u001b[A\n",
            "Iteration:  62% 283/453 [05:30<03:18,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 284/453 [05:31<03:17,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 285/453 [05:32<03:15,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 286/453 [05:33<03:14,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 287/453 [05:35<03:13,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 288/453 [05:36<03:12,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 289/453 [05:37<03:12,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 290/453 [05:38<03:10,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 291/453 [05:39<03:09,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 292/453 [05:40<03:07,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 293/453 [05:42<03:06,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 294/453 [05:43<03:05,  1.16s/it]\u001b[A\n",
            "Iteration:  65% 295/453 [05:44<03:03,  1.16s/it]\u001b[A\n",
            "Iteration:  65% 296/453 [05:45<03:02,  1.16s/it]\u001b[A\n",
            "Iteration:  66% 297/453 [05:46<03:01,  1.16s/it]\u001b[A\n",
            "Iteration:  66% 298/453 [05:47<03:00,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 299/453 [05:49<02:59,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 300/453 [05:50<02:58,  1.16s/it]\u001b[A\n",
            "Iteration:  66% 301/453 [05:51<02:57,  1.16s/it]\u001b[A\n",
            "Iteration:  67% 302/453 [05:52<02:55,  1.16s/it]\u001b[A\n",
            "Iteration:  67% 303/453 [05:53<02:54,  1.16s/it]\u001b[A\n",
            "Iteration:  67% 304/453 [05:54<02:53,  1.16s/it]\u001b[A\n",
            "Iteration:  67% 305/453 [05:56<02:52,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 306/453 [05:57<02:51,  1.16s/it]\u001b[A\n",
            "Iteration:  68% 307/453 [05:58<02:50,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 308/453 [05:59<02:48,  1.16s/it]\u001b[A\n",
            "Iteration:  68% 309/453 [06:00<02:47,  1.16s/it]\u001b[A\n",
            "Iteration:  68% 310/453 [06:01<02:46,  1.16s/it]\u001b[A\n",
            "Iteration:  69% 311/453 [06:03<02:45,  1.16s/it]\u001b[A\n",
            "Iteration:  69% 312/453 [06:04<02:44,  1.16s/it]\u001b[A\n",
            "Iteration:  69% 313/453 [06:05<02:42,  1.16s/it]\u001b[A\n",
            "Iteration:  69% 314/453 [06:06<02:41,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 315/453 [06:07<02:40,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 316/453 [06:08<02:39,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 317/453 [06:10<02:38,  1.16s/it]\u001b[A\n",
            "Iteration:  70% 318/453 [06:11<02:37,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 319/453 [06:12<02:36,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 320/453 [06:13<02:35,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 321/453 [06:14<02:34,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 322/453 [06:15<02:33,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 323/453 [06:17<02:31,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 324/453 [06:18<02:30,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 325/453 [06:19<02:29,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 326/453 [06:20<02:27,  1.16s/it]\u001b[A\n",
            "Iteration:  72% 327/453 [06:21<02:26,  1.16s/it]\u001b[A\n",
            "Iteration:  72% 328/453 [06:22<02:26,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 329/453 [06:24<02:24,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 330/453 [06:25<02:23,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 331/453 [06:26<02:22,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 332/453 [06:27<02:21,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 333/453 [06:28<02:19,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 334/453 [06:29<02:19,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 335/453 [06:31<02:17,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 336/453 [06:32<02:16,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 337/453 [06:33<02:15,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 338/453 [06:34<02:14,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 339/453 [06:35<02:13,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 340/453 [06:36<02:11,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 341/453 [06:38<02:10,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 342/453 [06:39<02:09,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 343/453 [06:40<02:08,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 344/453 [06:41<02:07,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 345/453 [06:42<02:05,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 346/453 [06:43<02:04,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 347/453 [06:45<02:03,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 348/453 [06:46<02:02,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 349/453 [06:47<02:01,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 350/453 [06:48<02:00,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 351/453 [06:49<01:59,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 352/453 [06:50<01:58,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 353/453 [06:52<01:56,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 354/453 [06:53<01:55,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 355/453 [06:54<01:54,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 356/453 [06:55<01:53,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 357/453 [06:56<01:52,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 358/453 [06:57<01:51,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 359/453 [06:59<01:50,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 360/453 [07:00<01:48,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 361/453 [07:01<01:48,  1.18s/it]\u001b[A\n",
            "Iteration:  80% 362/453 [07:02<01:46,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 363/453 [07:03<01:45,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 364/453 [07:04<01:43,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 365/453 [07:06<01:42,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 366/453 [07:07<01:41,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 367/453 [07:08<01:40,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 368/453 [07:09<01:39,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 369/453 [07:10<01:38,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 370/453 [07:11<01:36,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 371/453 [07:13<01:35,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 372/453 [07:14<01:34,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 373/453 [07:15<01:33,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 374/453 [07:16<01:32,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 375/453 [07:17<01:31,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 376/453 [07:19<01:30,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 377/453 [07:20<01:29,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 378/453 [07:21<01:27,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 379/453 [07:22<01:26,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 380/453 [07:23<01:25,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 381/453 [07:24<01:24,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 382/453 [07:26<01:23,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 383/453 [07:27<01:21,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 384/453 [07:28<01:20,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 385/453 [07:29<01:19,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 386/453 [07:30<01:18,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 387/453 [07:31<01:17,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 388/453 [07:33<01:15,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 389/453 [07:34<01:14,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 390/453 [07:35<01:13,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 391/453 [07:36<01:12,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 392/453 [07:37<01:11,  1.18s/it]\u001b[A\n",
            "Iteration:  87% 393/453 [07:38<01:10,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 394/453 [07:40<01:09,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 395/453 [07:41<01:08,  1.18s/it]\u001b[A\n",
            "Iteration:  87% 396/453 [07:42<01:07,  1.18s/it]\u001b[A\n",
            "Iteration:  88% 397/453 [07:43<01:05,  1.18s/it]\u001b[A\n",
            "Iteration:  88% 398/453 [07:44<01:04,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 399/453 [07:45<01:03,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 400/453 [07:47<01:01,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 401/453 [07:48<01:00,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 402/453 [07:49<00:59,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 403/453 [07:50<00:58,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 404/453 [07:51<00:57,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 405/453 [07:52<00:56,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 406/453 [07:54<00:54,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 407/453 [07:55<00:53,  1.16s/it]\u001b[A\n",
            "Iteration:  90% 408/453 [07:56<00:52,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 409/453 [07:57<00:51,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 410/453 [07:58<00:50,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 411/453 [07:59<00:49,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 412/453 [08:01<00:48,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 413/453 [08:02<00:46,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 414/453 [08:03<00:45,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 415/453 [08:04<00:44,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 416/453 [08:05<00:43,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 417/453 [08:06<00:42,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 418/453 [08:08<00:40,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 419/453 [08:09<00:39,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 420/453 [08:10<00:38,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 421/453 [08:11<00:37,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 422/453 [08:12<00:36,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 423/453 [08:13<00:35,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 424/453 [08:15<00:33,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 425/453 [08:16<00:32,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 426/453 [08:17<00:31,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 427/453 [08:18<00:30,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 428/453 [08:19<00:29,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 429/453 [08:20<00:28,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 430/453 [08:22<00:26,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 431/453 [08:23<00:25,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 432/453 [08:24<00:24,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 433/453 [08:25<00:23,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 434/453 [08:26<00:22,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 435/453 [08:28<00:21,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 436/453 [08:29<00:19,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 437/453 [08:30<00:18,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 438/453 [08:31<00:17,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 439/453 [08:32<00:16,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 440/453 [08:33<00:15,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 441/453 [08:34<00:13,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 442/453 [08:36<00:12,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 443/453 [08:37<00:11,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 444/453 [08:38<00:10,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 445/453 [08:39<00:09,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 446/453 [08:40<00:08,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 447/453 [08:41<00:06,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 448/453 [08:43<00:05,  1.17s/it]\u001b[A\n",
            "Iteration:  99% 449/453 [08:44<00:04,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 450/453 [08:45<00:03,  1.16s/it]\u001b[A\n",
            "Iteration: 100% 451/453 [08:46<00:02,  1.16s/it]\u001b[A\n",
            "Iteration: 100% 452/453 [08:47<00:01,  1.16s/it]\u001b[A\n",
            "Iteration: 100% 453/453 [08:48<00:00,  1.14s/it]\u001b[A\n",
            "Epoch:  67% 2/3 [17:36<08:48, 528.34s/it]\n",
            "Iteration:   0% 0/453 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/453 [00:01<08:41,  1.15s/it]\u001b[A\n",
            "Iteration:   0% 2/453 [00:02<08:42,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 3/453 [00:03<08:42,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 4/453 [00:04<08:41,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 5/453 [00:05<08:40,  1.16s/it]\u001b[A\n",
            "Iteration:   1% 6/453 [00:06<08:40,  1.17s/it]\u001b[A\n",
            "Iteration:   2% 7/453 [00:08<08:39,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 8/453 [00:09<08:38,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 9/453 [00:10<08:37,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 10/453 [00:11<08:35,  1.16s/it]\u001b[A\n",
            "Iteration:   2% 11/453 [00:12<08:34,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 12/453 [00:13<08:32,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 13/453 [00:15<08:30,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 14/453 [00:16<08:29,  1.16s/it]\u001b[A\n",
            "Iteration:   3% 15/453 [00:17<08:28,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 16/453 [00:18<08:27,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 17/453 [00:19<08:26,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 18/453 [00:20<08:26,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 19/453 [00:22<08:24,  1.16s/it]\u001b[A\n",
            "Iteration:   4% 20/453 [00:23<08:23,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 21/453 [00:24<08:22,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 22/453 [00:25<08:22,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 23/453 [00:26<08:20,  1.16s/it]\u001b[A\n",
            "Iteration:   5% 24/453 [00:27<08:19,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 25/453 [00:29<08:18,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 26/453 [00:30<08:17,  1.16s/it]\u001b[A\n",
            "Iteration:   6% 27/453 [00:31<08:18,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 28/453 [00:32<08:16,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 29/453 [00:33<08:16,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 30/453 [00:34<08:14,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 31/453 [00:36<08:13,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 32/453 [00:37<08:12,  1.17s/it]\u001b[A\n",
            "Iteration:   7% 33/453 [00:38<08:10,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 34/453 [00:39<08:08,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 35/453 [00:40<08:07,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 36/453 [00:41<08:05,  1.16s/it]\u001b[A\n",
            "Iteration:   8% 37/453 [00:43<08:04,  1.17s/it]\u001b[A\n",
            "Iteration:   8% 38/453 [00:44<08:03,  1.17s/it]\u001b[A\n",
            "Iteration:   9% 39/453 [00:45<08:02,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 40/453 [00:46<08:00,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 41/453 [00:47<07:59,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 42/453 [00:48<07:57,  1.16s/it]\u001b[A\n",
            "Iteration:   9% 43/453 [00:50<07:57,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 44/453 [00:51<07:56,  1.16s/it]\u001b[A\n",
            "Iteration:  10% 45/453 [00:52<07:55,  1.17s/it]\u001b[A\n",
            "Iteration:  10% 46/453 [00:53<07:54,  1.17s/it]\u001b[A\n",
            "Iteration:  10% 47/453 [00:54<07:54,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 48/453 [00:55<07:52,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 49/453 [00:57<07:51,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 50/453 [00:58<07:50,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 51/453 [00:59<07:49,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 52/453 [01:00<07:49,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 53/453 [01:01<07:48,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 54/453 [01:02<07:46,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 55/453 [01:04<07:45,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 56/453 [01:05<07:44,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 57/453 [01:06<07:42,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 58/453 [01:07<07:41,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 59/453 [01:08<07:40,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 60/453 [01:09<07:40,  1.17s/it]\u001b[A\n",
            "Iteration:  13% 61/453 [01:11<07:39,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 62/453 [01:12<07:37,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 63/453 [01:13<07:35,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 64/453 [01:14<07:34,  1.17s/it]\u001b[A\n",
            "Iteration:  14% 65/453 [01:15<07:32,  1.17s/it]\u001b[A\n",
            "Iteration:  15% 66/453 [01:16<07:30,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 67/453 [01:18<07:28,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 68/453 [01:19<07:28,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 69/453 [01:20<07:26,  1.16s/it]\u001b[A\n",
            "Iteration:  15% 70/453 [01:21<07:26,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 71/453 [01:22<07:25,  1.17s/it]\u001b[A\n",
            "Iteration:  16% 72/453 [01:23<07:23,  1.16s/it]\u001b[A\n",
            "Iteration:  16% 73/453 [01:25<07:22,  1.16s/it]\u001b[A\n",
            "Iteration:  16% 74/453 [01:26<07:20,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 75/453 [01:27<07:20,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 76/453 [01:28<07:19,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 77/453 [01:29<07:17,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 78/453 [01:30<07:17,  1.17s/it]\u001b[A\n",
            "Iteration:  17% 79/453 [01:32<07:16,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 80/453 [01:33<07:15,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 81/453 [01:34<07:14,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 82/453 [01:35<07:13,  1.17s/it]\u001b[A\n",
            "Iteration:  18% 83/453 [01:36<07:13,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 84/453 [01:37<07:11,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 85/453 [01:39<07:10,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 86/453 [01:40<07:08,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 87/453 [01:41<07:07,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 88/453 [01:42<07:04,  1.16s/it]\u001b[A\n",
            "Iteration:  20% 89/453 [01:43<07:04,  1.16s/it]\u001b[A\n",
            "Iteration:  20% 90/453 [01:44<07:03,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 91/453 [01:46<07:02,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 92/453 [01:47<07:01,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 93/453 [01:48<06:59,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 94/453 [01:49<06:58,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 95/453 [01:50<06:57,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 96/453 [01:51<06:55,  1.16s/it]\u001b[A\n",
            "Iteration:  21% 97/453 [01:53<06:54,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 98/453 [01:54<06:53,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 99/453 [01:55<06:51,  1.16s/it]\u001b[A\n",
            "Iteration:  22% 100/453 [01:56<06:51,  1.17s/it]\u001b[A\n",
            "Iteration:  22% 101/453 [01:57<06:50,  1.17s/it]\u001b[A\n",
            "Iteration:  23% 102/453 [01:58<06:49,  1.17s/it]\u001b[A\n",
            "Iteration:  23% 103/453 [02:00<06:47,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 104/453 [02:01<06:46,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 105/453 [02:02<06:45,  1.16s/it]\u001b[A\n",
            "Iteration:  23% 106/453 [02:03<06:44,  1.17s/it]\u001b[A\n",
            "Iteration:  24% 107/453 [02:04<06:44,  1.17s/it]\u001b[A\n",
            "Iteration:  24% 108/453 [02:05<06:42,  1.17s/it]\u001b[A\n",
            "Iteration:  24% 109/453 [02:07<06:42,  1.17s/it]\u001b[A\n",
            "Iteration:  24% 110/453 [02:08<06:40,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 111/453 [02:09<06:39,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 112/453 [02:10<06:39,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 113/453 [02:11<06:37,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 114/453 [02:12<06:36,  1.17s/it]\u001b[A\n",
            "Iteration:  25% 115/453 [02:14<06:34,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 116/453 [02:15<06:32,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 117/453 [02:16<06:31,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 118/453 [02:17<06:30,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 119/453 [02:18<06:29,  1.17s/it]\u001b[A\n",
            "Iteration:  26% 120/453 [02:19<06:27,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 121/453 [02:21<06:26,  1.16s/it]\u001b[A\n",
            "Iteration:  27% 122/453 [02:22<06:25,  1.17s/it]\u001b[A\n",
            "Iteration:  27% 123/453 [02:23<06:24,  1.16s/it]\u001b[A\n",
            "Iteration:  27% 124/453 [02:24<06:24,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 125/453 [02:25<06:22,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 126/453 [02:26<06:21,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 127/453 [02:28<06:19,  1.17s/it]\u001b[A\n",
            "Iteration:  28% 128/453 [02:29<06:18,  1.16s/it]\u001b[A\n",
            "Iteration:  28% 129/453 [02:30<06:17,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 130/453 [02:31<06:16,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 131/453 [02:32<06:14,  1.16s/it]\u001b[A\n",
            "Iteration:  29% 132/453 [02:33<06:14,  1.17s/it]\u001b[A\n",
            "Iteration:  29% 133/453 [02:35<06:13,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 134/453 [02:36<06:12,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 135/453 [02:37<06:11,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 136/453 [02:38<06:09,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 137/453 [02:39<06:09,  1.17s/it]\u001b[A\n",
            "Iteration:  30% 138/453 [02:40<06:08,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 139/453 [02:42<06:07,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 140/453 [02:43<06:05,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 141/453 [02:44<06:04,  1.17s/it]\u001b[A\n",
            "Iteration:  31% 142/453 [02:45<06:03,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 143/453 [02:46<06:01,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 144/453 [02:47<06:00,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 145/453 [02:49<05:59,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 146/453 [02:50<06:00,  1.17s/it]\u001b[A\n",
            "Iteration:  32% 147/453 [02:51<05:58,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 148/453 [02:52<05:57,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 149/453 [02:53<05:55,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 150/453 [02:54<05:54,  1.17s/it]\u001b[A\n",
            "Iteration:  33% 151/453 [02:56<05:53,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 152/453 [02:57<05:52,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 153/453 [02:58<05:50,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 154/453 [02:59<05:49,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 155/453 [03:00<05:47,  1.17s/it]\u001b[A\n",
            "Iteration:  34% 156/453 [03:01<05:46,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 157/453 [03:03<05:45,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 158/453 [03:04<05:45,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 159/453 [03:05<05:43,  1.17s/it]\u001b[A\n",
            "Iteration:  35% 160/453 [03:06<05:43,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 161/453 [03:07<05:41,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 162/453 [03:08<05:39,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 163/453 [03:10<05:39,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 164/453 [03:11<05:37,  1.17s/it]\u001b[A\n",
            "Iteration:  36% 165/453 [03:12<05:36,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 166/453 [03:13<05:35,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 167/453 [03:14<05:33,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 168/453 [03:16<05:32,  1.17s/it]\u001b[A\n",
            "Iteration:  37% 169/453 [03:17<05:32,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 170/453 [03:18<05:31,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 171/453 [03:19<05:29,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 172/453 [03:20<05:27,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 173/453 [03:21<05:26,  1.17s/it]\u001b[A\n",
            "Iteration:  38% 174/453 [03:23<05:25,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 175/453 [03:24<05:25,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 176/453 [03:25<05:23,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 177/453 [03:26<05:21,  1.17s/it]\u001b[A\n",
            "Iteration:  39% 178/453 [03:27<05:20,  1.17s/it]\u001b[A\n",
            "Iteration:  40% 179/453 [03:28<05:19,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 180/453 [03:30<05:17,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 181/453 [03:31<05:16,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 182/453 [03:32<05:14,  1.16s/it]\u001b[A\n",
            "Iteration:  40% 183/453 [03:33<05:14,  1.16s/it]\u001b[A\n",
            "Iteration:  41% 184/453 [03:34<05:13,  1.17s/it]\u001b[A\n",
            "Iteration:  41% 185/453 [03:35<05:12,  1.16s/it]\u001b[A\n",
            "Iteration:  41% 186/453 [03:36<05:11,  1.16s/it]\u001b[A\n",
            "Iteration:  41% 187/453 [03:38<05:09,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 188/453 [03:39<05:08,  1.16s/it]\u001b[A\n",
            "Iteration:  42% 189/453 [03:40<05:07,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 190/453 [03:41<05:06,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 191/453 [03:42<05:05,  1.17s/it]\u001b[A\n",
            "Iteration:  42% 192/453 [03:43<05:04,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 193/453 [03:45<05:03,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 194/453 [03:46<05:01,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 195/453 [03:47<05:00,  1.17s/it]\u001b[A\n",
            "Iteration:  43% 196/453 [03:48<04:59,  1.16s/it]\u001b[A\n",
            "Iteration:  43% 197/453 [03:49<04:58,  1.16s/it]\u001b[A\n",
            "Iteration:  44% 198/453 [03:50<04:56,  1.16s/it]\u001b[A\n",
            "Iteration:  44% 199/453 [03:52<04:56,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 200/453 [03:53<04:55,  1.17s/it]\u001b[A\n",
            "Iteration:  44% 201/453 [03:54<04:54,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 202/453 [03:55<04:52,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 203/453 [03:56<04:51,  1.16s/it]\u001b[A\n",
            "Iteration:  45% 204/453 [03:57<04:49,  1.16s/it]\u001b[A\n",
            "Iteration:  45% 205/453 [03:59<04:49,  1.17s/it]\u001b[A\n",
            "Iteration:  45% 206/453 [04:00<04:47,  1.16s/it]\u001b[A\n",
            "Iteration:  46% 207/453 [04:01<04:46,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 208/453 [04:02<04:45,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 209/453 [04:03<04:44,  1.17s/it]\u001b[A\n",
            "Iteration:  46% 210/453 [04:04<04:43,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 211/453 [04:06<04:42,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 212/453 [04:07<04:40,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 213/453 [04:08<04:39,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 214/453 [04:09<04:38,  1.17s/it]\u001b[A\n",
            "Iteration:  47% 215/453 [04:10<04:37,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 216/453 [04:11<04:36,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 217/453 [04:13<04:35,  1.17s/it]\u001b[A\n",
            "Iteration:  48% 218/453 [04:14<04:33,  1.16s/it]\u001b[A\n",
            "Iteration:  48% 219/453 [04:15<04:32,  1.16s/it]\u001b[A\n",
            "Iteration:  49% 220/453 [04:16<04:31,  1.17s/it]\u001b[A\n",
            "Iteration:  49% 221/453 [04:17<04:30,  1.16s/it]\u001b[A\n",
            "Iteration:  49% 222/453 [04:18<04:29,  1.16s/it]\u001b[A\n",
            "Iteration:  49% 223/453 [04:20<04:27,  1.16s/it]\u001b[A\n",
            "Iteration:  49% 224/453 [04:21<04:26,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 225/453 [04:22<04:25,  1.17s/it]\u001b[A\n",
            "Iteration:  50% 226/453 [04:23<04:24,  1.16s/it]\u001b[A\n",
            "Iteration:  50% 227/453 [04:24<04:22,  1.16s/it]\u001b[A\n",
            "Iteration:  50% 228/453 [04:25<04:21,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 229/453 [04:27<04:20,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 230/453 [04:28<04:19,  1.16s/it]\u001b[A\n",
            "Iteration:  51% 231/453 [04:29<04:19,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 232/453 [04:30<04:17,  1.17s/it]\u001b[A\n",
            "Iteration:  51% 233/453 [04:31<04:16,  1.17s/it]\u001b[A\n",
            "Iteration:  52% 234/453 [04:32<04:17,  1.17s/it]\u001b[A\n",
            "Iteration:  52% 235/453 [04:34<04:17,  1.18s/it]\u001b[A\n",
            "Iteration:  52% 236/453 [04:35<04:14,  1.17s/it]\u001b[A\n",
            "Iteration:  52% 237/453 [04:36<04:13,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 238/453 [04:37<04:12,  1.18s/it]\u001b[A\n",
            "Iteration:  53% 239/453 [04:38<04:11,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 240/453 [04:40<04:09,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 241/453 [04:41<04:07,  1.17s/it]\u001b[A\n",
            "Iteration:  53% 242/453 [04:42<04:06,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 243/453 [04:43<04:05,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 244/453 [04:44<04:04,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 245/453 [04:45<04:02,  1.17s/it]\u001b[A\n",
            "Iteration:  54% 246/453 [04:47<04:03,  1.18s/it]\u001b[A\n",
            "Iteration:  55% 247/453 [04:48<04:01,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 248/453 [04:49<03:59,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 249/453 [04:50<03:58,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 250/453 [04:51<03:57,  1.17s/it]\u001b[A\n",
            "Iteration:  55% 251/453 [04:52<03:55,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 252/453 [04:54<03:54,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 253/453 [04:55<03:53,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 254/453 [04:56<03:52,  1.17s/it]\u001b[A\n",
            "Iteration:  56% 255/453 [04:57<03:51,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 256/453 [04:58<03:50,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 257/453 [04:59<03:49,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 258/453 [05:01<03:48,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 259/453 [05:02<03:47,  1.17s/it]\u001b[A\n",
            "Iteration:  57% 260/453 [05:03<03:46,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 261/453 [05:04<03:45,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 262/453 [05:05<03:44,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 263/453 [05:06<03:42,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 264/453 [05:08<03:41,  1.17s/it]\u001b[A\n",
            "Iteration:  58% 265/453 [05:09<03:40,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 266/453 [05:10<03:39,  1.17s/it]\u001b[A\n",
            "Iteration:  59% 267/453 [05:11<03:38,  1.18s/it]\u001b[A\n",
            "Iteration:  59% 268/453 [05:12<03:38,  1.18s/it]\u001b[A\n",
            "Iteration:  59% 269/453 [05:13<03:36,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 270/453 [05:15<03:34,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 271/453 [05:16<03:32,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 272/453 [05:17<03:32,  1.18s/it]\u001b[A\n",
            "Iteration:  60% 273/453 [05:18<03:31,  1.17s/it]\u001b[A\n",
            "Iteration:  60% 274/453 [05:19<03:29,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 275/453 [05:20<03:28,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 276/453 [05:22<03:27,  1.17s/it]\u001b[A\n",
            "Iteration:  61% 277/453 [05:23<03:27,  1.18s/it]\u001b[A\n",
            "Iteration:  61% 278/453 [05:24<03:25,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 279/453 [05:25<03:24,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 280/453 [05:26<03:22,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 281/453 [05:28<03:21,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 282/453 [05:29<03:20,  1.17s/it]\u001b[A\n",
            "Iteration:  62% 283/453 [05:30<03:19,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 284/453 [05:31<03:17,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 285/453 [05:32<03:16,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 286/453 [05:33<03:15,  1.17s/it]\u001b[A\n",
            "Iteration:  63% 287/453 [05:35<03:14,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 288/453 [05:36<03:13,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 289/453 [05:37<03:12,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 290/453 [05:38<03:10,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 291/453 [05:39<03:09,  1.17s/it]\u001b[A\n",
            "Iteration:  64% 292/453 [05:40<03:09,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 293/453 [05:42<03:07,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 294/453 [05:43<03:05,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 295/453 [05:44<03:04,  1.17s/it]\u001b[A\n",
            "Iteration:  65% 296/453 [05:45<03:03,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 297/453 [05:46<03:02,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 298/453 [05:47<03:01,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 299/453 [05:49<02:59,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 300/453 [05:50<02:59,  1.17s/it]\u001b[A\n",
            "Iteration:  66% 301/453 [05:51<02:57,  1.17s/it]\u001b[A\n",
            "Iteration:  67% 302/453 [05:52<02:56,  1.17s/it]\u001b[A\n",
            "Iteration:  67% 303/453 [05:53<02:54,  1.16s/it]\u001b[A\n",
            "Iteration:  67% 304/453 [05:54<02:53,  1.16s/it]\u001b[A\n",
            "Iteration:  67% 305/453 [05:56<02:52,  1.16s/it]\u001b[A\n",
            "Iteration:  68% 306/453 [05:57<02:51,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 307/453 [05:58<02:50,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 308/453 [05:59<02:48,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 309/453 [06:00<02:47,  1.17s/it]\u001b[A\n",
            "Iteration:  68% 310/453 [06:01<02:46,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 311/453 [06:03<02:45,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 312/453 [06:04<02:44,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 313/453 [06:05<02:43,  1.17s/it]\u001b[A\n",
            "Iteration:  69% 314/453 [06:06<02:42,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 315/453 [06:07<02:41,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 316/453 [06:08<02:39,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 317/453 [06:10<02:38,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 318/453 [06:11<02:38,  1.17s/it]\u001b[A\n",
            "Iteration:  70% 319/453 [06:12<02:36,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 320/453 [06:13<02:35,  1.17s/it]\u001b[A\n",
            "Iteration:  71% 321/453 [06:14<02:35,  1.18s/it]\u001b[A\n",
            "Iteration:  71% 322/453 [06:15<02:34,  1.18s/it]\u001b[A\n",
            "Iteration:  71% 323/453 [06:17<02:33,  1.18s/it]\u001b[A\n",
            "Iteration:  72% 324/453 [06:18<02:31,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 325/453 [06:19<02:30,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 326/453 [06:20<02:28,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 327/453 [06:21<02:27,  1.17s/it]\u001b[A\n",
            "Iteration:  72% 328/453 [06:23<02:27,  1.18s/it]\u001b[A\n",
            "Iteration:  73% 329/453 [06:24<02:26,  1.18s/it]\u001b[A\n",
            "Iteration:  73% 330/453 [06:25<02:24,  1.18s/it]\u001b[A\n",
            "Iteration:  73% 331/453 [06:26<02:23,  1.17s/it]\u001b[A\n",
            "Iteration:  73% 332/453 [06:27<02:21,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 333/453 [06:28<02:20,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 334/453 [06:30<02:19,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 335/453 [06:31<02:18,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 336/453 [06:32<02:17,  1.17s/it]\u001b[A\n",
            "Iteration:  74% 337/453 [06:33<02:15,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 338/453 [06:34<02:14,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 339/453 [06:35<02:13,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 340/453 [06:37<02:11,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 341/453 [06:38<02:10,  1.17s/it]\u001b[A\n",
            "Iteration:  75% 342/453 [06:39<02:09,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 343/453 [06:40<02:08,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 344/453 [06:41<02:07,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 345/453 [06:42<02:06,  1.17s/it]\u001b[A\n",
            "Iteration:  76% 346/453 [06:44<02:05,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 347/453 [06:45<02:04,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 348/453 [06:46<02:02,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 349/453 [06:47<02:01,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 350/453 [06:48<02:00,  1.17s/it]\u001b[A\n",
            "Iteration:  77% 351/453 [06:49<01:59,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 352/453 [06:51<01:57,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 353/453 [06:52<01:57,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 354/453 [06:53<01:55,  1.17s/it]\u001b[A\n",
            "Iteration:  78% 355/453 [06:54<01:54,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 356/453 [06:55<01:53,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 357/453 [06:56<01:51,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 358/453 [06:58<01:50,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 359/453 [06:59<01:49,  1.17s/it]\u001b[A\n",
            "Iteration:  79% 360/453 [07:00<01:48,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 361/453 [07:01<01:47,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 362/453 [07:02<01:46,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 363/453 [07:03<01:45,  1.17s/it]\u001b[A\n",
            "Iteration:  80% 364/453 [07:05<01:44,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 365/453 [07:06<01:42,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 366/453 [07:07<01:41,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 367/453 [07:08<01:40,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 368/453 [07:09<01:39,  1.17s/it]\u001b[A\n",
            "Iteration:  81% 369/453 [07:10<01:38,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 370/453 [07:12<01:37,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 371/453 [07:13<01:36,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 372/453 [07:14<01:34,  1.17s/it]\u001b[A\n",
            "Iteration:  82% 373/453 [07:15<01:33,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 374/453 [07:16<01:32,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 375/453 [07:18<01:31,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 376/453 [07:19<01:30,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 377/453 [07:20<01:28,  1.17s/it]\u001b[A\n",
            "Iteration:  83% 378/453 [07:21<01:27,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 379/453 [07:22<01:26,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 380/453 [07:23<01:25,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 381/453 [07:25<01:24,  1.17s/it]\u001b[A\n",
            "Iteration:  84% 382/453 [07:26<01:23,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 383/453 [07:27<01:21,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 384/453 [07:28<01:20,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 385/453 [07:29<01:19,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 386/453 [07:30<01:18,  1.17s/it]\u001b[A\n",
            "Iteration:  85% 387/453 [07:32<01:17,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 388/453 [07:33<01:16,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 389/453 [07:34<01:14,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 390/453 [07:35<01:13,  1.17s/it]\u001b[A\n",
            "Iteration:  86% 391/453 [07:36<01:12,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 392/453 [07:37<01:11,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 393/453 [07:39<01:10,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 394/453 [07:40<01:09,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 395/453 [07:41<01:07,  1.17s/it]\u001b[A\n",
            "Iteration:  87% 396/453 [07:42<01:06,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 397/453 [07:43<01:05,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 398/453 [07:44<01:04,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 399/453 [07:46<01:03,  1.17s/it]\u001b[A\n",
            "Iteration:  88% 400/453 [07:47<01:02,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 401/453 [07:48<01:00,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 402/453 [07:49<00:59,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 403/453 [07:50<00:58,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 404/453 [07:51<00:57,  1.17s/it]\u001b[A\n",
            "Iteration:  89% 405/453 [07:53<00:56,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 406/453 [07:54<00:55,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 407/453 [07:55<00:53,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 408/453 [07:56<00:52,  1.17s/it]\u001b[A\n",
            "Iteration:  90% 409/453 [07:57<00:51,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 410/453 [07:58<00:50,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 411/453 [08:00<00:49,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 412/453 [08:01<00:47,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 413/453 [08:02<00:46,  1.17s/it]\u001b[A\n",
            "Iteration:  91% 414/453 [08:03<00:45,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 415/453 [08:04<00:44,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 416/453 [08:05<00:43,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 417/453 [08:07<00:42,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 418/453 [08:08<00:40,  1.17s/it]\u001b[A\n",
            "Iteration:  92% 419/453 [08:09<00:39,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 420/453 [08:10<00:38,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 421/453 [08:11<00:37,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 422/453 [08:12<00:36,  1.17s/it]\u001b[A\n",
            "Iteration:  93% 423/453 [08:14<00:35,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 424/453 [08:15<00:33,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 425/453 [08:16<00:32,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 426/453 [08:17<00:31,  1.16s/it]\u001b[A\n",
            "Iteration:  94% 427/453 [08:18<00:30,  1.17s/it]\u001b[A\n",
            "Iteration:  94% 428/453 [08:19<00:29,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 429/453 [08:21<00:28,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 430/453 [08:22<00:26,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 431/453 [08:23<00:25,  1.17s/it]\u001b[A\n",
            "Iteration:  95% 432/453 [08:24<00:24,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 433/453 [08:25<00:23,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 434/453 [08:26<00:22,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 435/453 [08:28<00:21,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 436/453 [08:29<00:19,  1.17s/it]\u001b[A\n",
            "Iteration:  96% 437/453 [08:30<00:18,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 438/453 [08:31<00:17,  1.16s/it]\u001b[A\n",
            "Iteration:  97% 439/453 [08:32<00:16,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 440/453 [08:33<00:15,  1.17s/it]\u001b[A\n",
            "Iteration:  97% 441/453 [08:35<00:13,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 442/453 [08:36<00:12,  1.16s/it]\u001b[A\n",
            "Iteration:  98% 443/453 [08:37<00:11,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 444/453 [08:38<00:10,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 445/453 [08:39<00:09,  1.17s/it]\u001b[A\n",
            "Iteration:  98% 446/453 [08:40<00:08,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 447/453 [08:42<00:06,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 448/453 [08:43<00:05,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 449/453 [08:44<00:04,  1.16s/it]\u001b[A\n",
            "Iteration:  99% 450/453 [08:45<00:03,  1.16s/it]\u001b[A\n",
            "Iteration: 100% 451/453 [08:46<00:02,  1.16s/it]\u001b[A\n",
            "Iteration: 100% 452/453 [08:47<00:01,  1.16s/it]\u001b[A\n",
            "Iteration: 100% 453/453 [08:49<00:00,  1.14s/it]\u001b[A\n",
            "Epoch: 100% 3/3 [26:26<00:00, 528.55s/it]\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   guid: dev-0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   tokens: [CLS] @ americana ##ir seriously , there aren ' t any rep ##s available to take phone calls ? even for platinum ? [SEP]\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_ids: 101 1030 25988 4313 5667 1010 2045 4995 1005 1056 2151 16360 2015 2800 2000 2202 3042 4455 1029 2130 2005 8899 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   guid: dev-1\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   tokens: [CLS] @ southwest ##air i will be calling someone on monday in customer relations . very disappointed in how i was treated by customer service ! [SEP]\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_ids: 101 1030 4943 11215 1045 2097 2022 4214 2619 2006 6928 1999 8013 4262 1012 2200 9364 1999 2129 1045 2001 5845 2011 8013 2326 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   guid: dev-2\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   tokens: [CLS] @ united does customer care have email or a phone rep that i can speak with ? we had so many issues they can ' t be placed in 2000 characters ? [SEP]\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_ids: 101 1030 2142 2515 8013 2729 2031 10373 2030 1037 3042 16360 2008 1045 2064 3713 2007 1029 2057 2018 2061 2116 3314 2027 2064 1005 1056 2022 2872 1999 2456 3494 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   guid: dev-3\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   tokens: [CLS] @ united why have airlines always told us they can ' t open a cabin door once it has been closed ? this plane has done it 3 times tonight . [SEP]\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_ids: 101 1030 2142 2339 2031 7608 2467 2409 2149 2027 2064 1005 1056 2330 1037 6644 2341 2320 2009 2038 2042 2701 1029 2023 4946 2038 2589 2009 1017 2335 3892 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   label: 1 (id = 1)\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   *** Example ***\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   guid: dev-4\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   tokens: [CLS] @ usa ##ir ##ways it is my fault but i bought online and i get twice why i buy twice it fault but i bo ##ug ##th ticket j ##fk to mcc ##are ##n and usa ##ir ##ways + + [SEP]\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_ids: 101 1030 3915 4313 14035 2009 2003 2026 6346 2021 1045 4149 3784 1998 1045 2131 3807 2339 1045 4965 3807 2009 6346 2021 1045 8945 15916 2705 7281 1046 24316 2000 23680 12069 2078 1998 3915 4313 14035 1009 1009 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   label: 0 (id = 0)\n",
            "03/27/2019 21:18:00 - INFO - __main__ -   ***** Running evaluation *****\n",
            "03/27/2019 21:18:00 - INFO - __main__ -     Num examples = 147\n",
            "03/27/2019 21:18:00 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 19/19 [00:04<00:00,  5.22it/s]\n",
            "03/27/2019 21:18:05 - INFO - __main__ -   ***** Eval results *****\n",
            "03/27/2019 21:18:05 - INFO - __main__ -     eval_accuracy = 0.8503401360544217\n",
            "03/27/2019 21:18:05 - INFO - __main__ -     eval_loss = 0.4185379944545658\n",
            "03/27/2019 21:18:05 - INFO - __main__ -     global_step = 1359\n",
            "03/27/2019 21:18:05 - INFO - __main__ -     loss = 0.1578007298408623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wiSvdQjn83GB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEcUA4zIDKii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GPT-2 "
      ]
    },
    {
      "metadata": {
        "id": "-lBNGWXHDOKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1395
        },
        "outputId": "5c8bab39-bdee-4e3f-9a2b-4bc387ef2c56"
      },
      "cell_type": "code",
      "source": [
        "!python run_gpt2.py"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "Namespace(batch_size=-1, length=-1, model_name_or_path='gpt2', nsamples=1, seed=0, temperature=1, top_k=0, unconditional=False)\n",
            "03/27/2019 21:37:05 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.pytorch_pretrained_bert/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/27/2019 21:37:05 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.pytorch_pretrained_bert/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/27/2019 21:37:05 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/27/2019 21:37:05 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.pytorch_pretrained_bert/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "03/27/2019 21:37:05 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   Model config {\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Model prompt >>> Who am I?\n",
            "100% 512/512 [00:14<00:00, 36.37it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "Take me to the QB!\n",
            "\n",
            "I came pregnant.\n",
            "\n",
            "\"I'm sure kids will be learning that to be a tad boy.\"<|endoftext|>Embed This Video On Your Site With This Html: Copy Embed code\n",
            "\n",
            "<iframe src=\"http://www.youjizz.com/videos/embed/242203\" frameborder=\"0\" style=\"width:100%; height:570px;\" scrolling=\"no\" allowtransparency=\"true\"></iframe><|endoftext|>As president-elect Donald Trump takes his dismantling of Barack Obama's health care law to the White House, it's hoped the good will of both generations won't override Kansas' ready-made attorney general. Getting somebody to poke fun at Trump for not keeping things on her from drawing up a repeal budget, following that up with a lame attempt by Sen. Susan Collins, R-Maine, to slash federal penalties on consumers who'd pay more for insurance through a Medicaid replacement, won't be a serious blow.\n",
            "\n",
            "That may be largely because it works; reaching a single-payer, single-payer healthcare system under Trump's new administration is not much of a magic bullet, and will make us reluctant to defund Planned Parenthood (which is what the Trump administration and the GOP are taking away as a price to pay for shutting it down), but Republicans weigh in as a country trying to find an adequate replacement.\n",
            "\n",
            "As Chairman of the Joint Committee on Taxation won't the CBO represent, Trump decides, it would give his political opponents less power to make legitimate criticism of AHCA available, or to oppose repeal legislation without sharing the pieces. \"Nobody can refuse to make it available. I mean, let's take a look here,\" said Senate Minority Leader Chuck Schumer, C-N.Y. (four of the sixteen Democratic senators who voted to repeal tax credits for having children care for themselves or their parents, by making free outreach to Russian entertainment hit shows about women injured in Syria). \"To me that means that's another piece of legislation from the Oval Office devoted exclusively to people who feel like we're firing them immediately.\"\n",
            "\n",
            "Senators are not Republican only because they represent nothing less than sovereignty on their party's national stage, but because they can break through entrenched partisan historical convictions in one moment with one substantive change they want to see voted upon in the next, and then repeat it because that moment feels unreal and basic.\n",
            "\n",
            "Now this is my heart, because every unelected bit of Republican leadership in the United States has a\n",
            "================================================================================\n",
            "Model prompt >>> Are you racist?\n",
            "100% 512/512 [00:13<00:00, 37.00it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            " [sniffs] I mean Christian, you know, those Christians, we have a holy book that is doctrinal moderate ... like a god? I think that it's time to come out and say like, wow, I just also see the long sweep animosity some people have toward Rule 2. . . but Franciscans also are notoriously violent there and they intend to be in charge at the end of each put. God is the judge of every place.\"\n",
            "\n",
            "The Patriot Leader has a tone similar to that of Rev. Carl Nelson's.\"Let me ask, is a right to exhibit racial and religious symbols on your property such a my God,\" he marveled, as one inquirer characterized those religious symbols.\"Jeb Talmage, you did this painting there, did you know?\" asked Gagnon.\n",
            "\n",
            "Through a crack at Trump for his response, Fitzgerald efficient rebuked; not once did Fitzgerald otherwise react. More snd just finished writing this post which the Journal overlooked due to focus and questionability and yet in this city transcribed the posting over 887 frustrating seconds in length and markedly. You know probably will not boo the level of the Presidential \"say turn\" for much money to basically pay for his utter disregard for our how we live. Not that there's anything wrong with pay-to-play or kicking Fauxouts. As a mother of one young daughter, pray for my voice. See \"Before this she left to go to Gothic, I looked up the alphabet to eat cookies For my Christian a Werewolf Wolf, and my Dog ; quote: Heaven my prayer\n",
            "\n",
            "I saw the scene these moronchildren were living in,\n",
            "\n",
            "they had room to spare from their loves.\n",
            "\n",
            "I couldn't make any conversation like a Walter White subject... Reject madmen of the City of Independence. [[End Post, 5/17/08] Hearing what Fitzgerald Barbara is saying, began to seem superfluous. Fitzgerald burst into a Googling of white people's black \"hypocrisy\" and found herself empowered with an insight understanding that the principle of \"black roots arrogant of white\" must be Austin McKetty's's vision of white supremacy, when it was the preeminence of a man and its map to totalitarianism, its sensibilities, its pleading what could be called \"impartiality to all our problems\" and the above alienation of white people. Within and of the Gentlemen's splashy rendition of the words, The Advocate adapted this core reading to help these white nationalist bast\n",
            "================================================================================\n",
            "Model prompt >>> Is this fair?\n",
            "100% 512/512 [00:14<00:00, 36.11it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "You will argue that if Nicholas is prepared to do an entire book on Islam and its historically-constituted medieval ways and standards, he could have so deliberately put himself front and center in colonial God-fearing and be-good with his Catholic colleagues.4 Yet if he accepts the whole premise of being professor of Canadian historical Islam (rather than economics or journalism in disguise), who would think this was the way to put a worldly self-promoter together?\n",
            "\n",
            "I addressed the question earlier in Sussan's 2004 critique of two well-known British historians of Islam: the Victorian English Professor Bertrand Russell, whose work had used Islam to advance the airing of the fabric of civilised events (1836 to 1837), and me710 provided here his list of possible concepts.\n",
            "\n",
            "Russell had argued that, through muslim theology, Christians if anything warranted ceasing their formally secular arbitrary treatments, i.e. accept a deassertion that an important remnant of the British settlement colonies (c.550–200 CE) emigrated through, in some form, Christmas Island to those whom they encountered as Christians. For the Sisters of Re since \"about 250,000 Pilgrim Crusaders settled here in a base not known for centuries,\" Pac-de-France updated this to \"about 500,000 Pilgrim Crusaders in X [97,000 to] 07,000 members.\"15 Pac-de-France designed the fairy tale as a part of literature he had been shielded from by the British, but given that this narration did not require Bible reading, these could barely be called \"Catholic\" (well, not exactly), and more likely were Hindu and/or Buddhist than Roman Catholicism. Game theory put Lucia into a \"godlike category,\" so if Lucia just lived entitled to monogamy \"outside of her communion with the Church,\" couldn't this be argued right? Let's put it this way: presumably, there has been a \"meaninglessness\" effect on a person's behaviour because she has not replaced someone else's godly self-assertiveness in the sense that as such she become a subject for others to seek her aid so time passes without proclamation. That was the context Amr Raz runs the research section of Nuncese, a British sectarian journal so notorious for problems of religious fat-shaming that Announcer, a Christian blogger OccupyPacific has written, \"Offer up Karen Odom's tales of ancestors profiling members who adopted Bible-reading sects.\"16 Pakiunga of Levant III has also\n",
            "================================================================================\n",
            "Model prompt >>> AI ethics\n",
            "100% 512/512 [00:13<00:00, 37.41it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". The existence of cancer detection studies (e.g., [18] or [17]) suggests that the potential dangers in these safer populations will evaporate rather quickly. In particular, the efficacy of computed tomography may be biased by the place [19] of these studies and non-observed changes by the candidate operative lacking previously published information on the gene or its regulatory networks in cancer. Furthermore, estimating tumor development and survival longer than postoperative monitoring will likely become extremely challenging. Moreover, the ability to exclude the lifesaving self-reported results provides experimental support for ≈80% of the reported gains (e.g., [20], [21]) for NCI MCI studies. As a lower perinatal ANC might make these results fail to account for a synergistic effect on navigation studies (e.g., [20], [22]), an unrealistic portrayal of the effects of MCI on cancer could result in underestimating the positive relations between legacy rate and age at disease onset [23] and potential hazards. Li et al. (2014) demonstrated long survival rates in three African American-born patients with both ALS and cancer (NCI) by using 1879 MCI patients with fatal DD-lacto-Lacturia (LL-D2) and 1876 MCI patients with occurrence premalignant tumour AIDS full safe lymphovirus (ME/CALV) prostate cancer in a small sample of non amenable MCI patients; blood samples revealed no association between blood levels of antioxidants [24] and survival. Here in contrast to Li et al. (2014) with black patients with ML-D2 (infected, vaccination massive) and blood levels within the protective threshold of serum # α of up to 1043 nmol/l, the causal link between familial lymphoma and high levels of CD4 to lymphocyte>/> prostate cancer shares a similar developmental path, witnes one Indian association resulted in a decrease in his 1677 MCI children (Zaple et al. 2016). However, theoretical differences may be relevant (e.g., the comparatedy of this study), doomed variable validity (e.g. the MD findings were applied only to enrolled movers), and distract from an end point of pathophysiological expertise (e.g., a control group of hip and cervical lung carcinoma or apoptomas) [25]. A high anticipatory rate might generate short term behavioral and clinical complications and may account for the less frequent outcome at metastasis such as meeting the\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"run_gpt2.py\", line 111, in <module>\n",
            "    run_model()\n",
            "  File \"run_gpt2.py\", line 88, in run_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EB_VWBQ-ESyX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}