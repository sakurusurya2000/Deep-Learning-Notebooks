{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_rnn_keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "v9ZgzXdmiWIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Char RNN\n",
        "\n",
        "In this notebook, we will go through basics of Char-RNN and use different datasets to create different RNNs.\n",
        "\n",
        "Here we will use [keras](https://keras.io  \"Keras Tutorial\").\n",
        "\n",
        "\n",
        "Everything is explained in-detail in [blog post](). This is notebook which replicates the result of blog and runs in colab. Enjoy!\n",
        "\n",
        "\n",
        "#### Run in Colab\n",
        "\n",
        "You can run this notebook in google colab.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dudeperf3ct/DL_notebooks/blob/master/RNN/char_rnn_keras.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "Here are some other very interesting results, [Cooking-Recipe](https://gist.github.com/nylki/1efbaa36635956d35bcc), [Obama-RNN](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0), [Bible-RNN](https://twitter.com/RNN_Bible), [Folk-music](https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music), [Learning Holiness](https://cpury.github.io/learning-holiness/), [AI Weirdness](http://aiweirdness.com/), [Auto-Generating Clickbait](https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)."
      ]
    },
    {
      "metadata": {
        "id": "xOND0u0Rp3R6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data"
      ]
    },
    {
      "metadata": {
        "id": "Ux9Ideiip5ym",
        "colab_type": "code",
        "outputId": "f61d5fe5-e3ed-4449-e6b0-61927c4ef7ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1431
        }
      },
      "cell_type": "code",
      "source": [
        "! wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" -P {'data/'}\n",
        "! wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\" -P {'data/'}\n",
        "! wget \"http://www.gutenberg.org/files/31100/31100.txt\" -P {'data/'}\n",
        "! wget \"http://www.gutenberg.org/cache/epub/29765/pg29765.txt\" -P {'data/'}\n",
        "! wget \"https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\" -P {'data/'}\n",
        "! wget \"https://raw.githubusercontent.com/mcleonard/pytorch-charRNN/master/data/anna.txt\" -P {'data/'}\n",
        "! wget \"https://raw.githubusercontent.com/samim23/obama-rnn/master/input.txt\" -P {'data/obama/'}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-16 15:09:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘data/input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-02-16 15:09:38 (13.4 MB/s) - ‘data/input.txt’ saved [1115394/1115394]\n",
            "\n",
            "--2019-02-16 15:09:40--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.85.133\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.85.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: ‘data/nietzsche.txt’\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K   787KB/s    in 0.7s    \n",
            "\n",
            "2019-02-16 15:09:42 (787 KB/s) - ‘data/nietzsche.txt’ saved [600901/600901]\n",
            "\n",
            "--2019-02-16 15:09:44--  http://www.gutenberg.org/files/31100/31100.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4454075 (4.2M) [text/plain]\n",
            "Saving to: ‘data/31100.txt’\n",
            "\n",
            "31100.txt           100%[===================>]   4.25M  1.01MB/s    in 6.1s    \n",
            "\n",
            "2019-02-16 15:09:51 (713 KB/s) - ‘data/31100.txt’ saved [4454075/4454075]\n",
            "\n",
            "--2019-02-16 15:09:53--  http://www.gutenberg.org/cache/epub/29765/pg29765.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28956348 (28M) [text/plain]\n",
            "Saving to: ‘data/pg29765.txt’\n",
            "\n",
            "pg29765.txt         100%[===================>]  27.61M  6.45MB/s    in 7.6s    \n",
            "\n",
            "2019-02-16 15:10:02 (3.62 MB/s) - ‘data/pg29765.txt’ saved [28956348/28956348]\n",
            "\n",
            "--2019-02-16 15:10:03--  https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 924745 (903K) [text/plain]\n",
            "Saving to: ‘data/speeches.txt’\n",
            "\n",
            "speeches.txt        100%[===================>] 903.07K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-02-16 15:10:04 (11.9 MB/s) - ‘data/speeches.txt’ saved [924745/924745]\n",
            "\n",
            "--2019-02-16 15:10:06--  https://raw.githubusercontent.com/mcleonard/pytorch-charRNN/master/data/anna.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2025486 (1.9M) [text/plain]\n",
            "Saving to: ‘data/anna.txt’\n",
            "\n",
            "anna.txt            100%[===================>]   1.93M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-02-16 15:10:06 (22.1 MB/s) - ‘data/anna.txt’ saved [2025486/2025486]\n",
            "\n",
            "--2019-02-16 15:10:09--  https://raw.githubusercontent.com/samim23/obama-rnn/master/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4250014 (4.1M) [text/plain]\n",
            "Saving to: ‘data/obama/input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.05M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-02-16 15:10:10 (38.5 MB/s) - ‘data/obama/input.txt’ saved [4250014/4250014]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AVq3IFekiRC2",
        "colab_type": "code",
        "outputId": "432a96f8-1d08-4c7a-c08b-319d8b2ed48a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import RNN, Dropout, TimeDistributed, Dense, Activation, Embedding, LSTM, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "B6y_vbig0rjh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shakespeare RNN"
      ]
    },
    {
      "metadata": {
        "id": "lXc3aOq8iZP6",
        "colab_type": "code",
        "outputId": "32bb7098-2638-4d2c-9aff-805538f8802d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/input.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2J8wSUiwc7I",
        "colab_type": "code",
        "outputId": "81ff06e8-8aa9-40f4-9038-c9dfa3c9b514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'l', 'a', 'w', 'y', 'c', 'n', 'q', 'd', 'i', 'f', ',', 'b', ':', 'h', 'v', '&', 'k', 'x', 't', '.', '?', ';', 'p', 'g', '\\n', 'o', ' ', '!', 'j', '3', '$', 'r', 'e', 'm', 'u', 's', 'z', '-', \"'\"}\n",
            "Length of Unique characters: 39\n",
            "Number of characters in data: 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p870Fa84iZTM",
        "colab_type": "code",
        "outputId": "27c2010c-40c9-4793-e9d9-b8b900ae8933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'l': 0, 'a': 1, 'w': 2, 'y': 3, 'c': 4, 'n': 5, 'q': 6, 'd': 7, 'i': 8, 'f': 9, ',': 10, 'b': 11, ':': 12, 'h': 13, 'v': 14, '&': 15, 'k': 16, 'x': 17, 't': 18, '.': 19, '?': 20, ';': 21, 'p': 22, 'g': 23, '\\n': 24, 'o': 25, ' ': 26, '!': 27, 'j': 28, '3': 29, '$': 30, 'r': 31, 'e': 32, 'm': 33, 'u': 34, 's': 35, 'z': 36, '-': 37, \"'\": 38}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VVse03FKum2s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 1"
      ]
    },
    {
      "metadata": {
        "id": "lywp7eVaiZZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0BgFtBjyutR",
        "colab_type": "code",
        "outputId": "3b9c27f6-1a53-484d-fddd-6e3123a6f997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9, 8, 31, 35, 18, 26, 4, 8, 18, 8, 36, 32, 5, 12, 24, 11, 32, 9, 25, 31, 32, 26, 2, 32, 26, 22, 31, 25, 4, 32, 32, 7, 26, 1, 5, 3, 26, 9, 34, 31] 40 27884\n",
            "[18] 1 27884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dofrv0NPFMS6",
        "colab_type": "code",
        "outputId": "de07888b-befd-431e-a1a3-654407868b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: first citizen:\n",
            "before we proceed any fur\n",
            "Output: t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AHZTjySMzQ8p",
        "colab_type": "code",
        "outputId": "3278e2e1-5b86-436a-99bc-667f4e28f1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen, vocab_size), dtype=np.int32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(sentences)) for c in sentences[i]]\n",
        "X = np.array(one_hot).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] (27884, 40, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mdyZILcM2sac",
        "colab_type": "code",
        "outputId": "5268b2cd-0b89-4dbe-a18f-fb23f6a7fd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(sentences), vocab_size), dtype=np.int32)\n",
        "one_hot = [to_categorical(next_chars[i], num_classes=vocab_size) for i in range(len(next_chars))]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (27884, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqI7hjQcJn1Q",
        "colab_type": "code",
        "outputId": "e65b4bc7-2df8-4c11-9b0c-9b4c9b284fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (22307, 40, 39) (22307, 39)\n",
            "Validation: (5577, 40, 39) (5577, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sKPwUN8MAbGa",
        "colab_type": "code",
        "outputId": "444e7aad-ab23-4c9d-ae20-31bd90b0c8ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen, vocab_size)))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "rmsprop = optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 40, 128)           86016     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 39)                5031      \n",
            "=================================================================\n",
            "Total params: 222,631\n",
            "Trainable params: 222,631\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "en2ymu7FQXdM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char2id[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wbz96t8rDVm2",
        "colab_type": "code",
        "outputId": "ab75e4e3-1519-4610-fd25-5085321db2b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1543
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 22307 samples, validate on 5577 samples\n",
            "Epoch 1/150\n",
            "22307/22307 [==============================] - 69s 3ms/step - loss: 3.1659 - val_loss: 3.0491\n",
            "Epoch 2/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 3.0288 - val_loss: 2.8523\n",
            "Epoch 3/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.8089 - val_loss: 2.5920\n",
            "Epoch 4/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.6490 - val_loss: 2.4698\n",
            "Epoch 5/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.5652 - val_loss: 2.4041\n",
            "Epoch 6/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.5177 - val_loss: 2.3565\n",
            "Epoch 7/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.4735 - val_loss: 2.3310\n",
            "Epoch 8/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.4430 - val_loss: 2.2964\n",
            "Epoch 9/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.4174 - val_loss: 2.2719\n",
            "Epoch 10/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.3910 - val_loss: 2.2525\n",
            "Epoch 11/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.3733 - val_loss: 2.2288\n",
            "Epoch 12/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.3608 - val_loss: 2.2173\n",
            "Epoch 13/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.3413 - val_loss: 2.1948\n",
            "Epoch 14/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.3237 - val_loss: 2.1726\n",
            "Epoch 15/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.3137 - val_loss: 2.1647\n",
            "Epoch 16/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2905 - val_loss: 2.1478\n",
            "Epoch 17/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2745 - val_loss: 2.1391\n",
            "Epoch 18/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2739 - val_loss: 2.1293\n",
            "Epoch 19/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2527 - val_loss: 2.1168\n",
            "Epoch 20/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2465 - val_loss: 2.1067\n",
            "Epoch 21/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.2456 - val_loss: 2.1117\n",
            "Epoch 22/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2291 - val_loss: 2.0949\n",
            "Epoch 23/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.2184 - val_loss: 2.0895\n",
            "Epoch 24/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.2057 - val_loss: 2.0821\n",
            "Epoch 25/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.1947 - val_loss: 2.0705\n",
            "Epoch 26/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.1817 - val_loss: 2.0662\n",
            "Epoch 27/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1784 - val_loss: 2.0668\n",
            "Epoch 28/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1732 - val_loss: 2.0545\n",
            "Epoch 29/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.1690 - val_loss: 2.0516\n",
            "Epoch 30/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1510 - val_loss: 2.0414\n",
            "Epoch 31/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.1393 - val_loss: 2.0385\n",
            "Epoch 32/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.1389 - val_loss: 2.0358\n",
            "Epoch 33/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1389 - val_loss: 2.0347\n",
            "Epoch 34/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1245 - val_loss: 2.0275\n",
            "Epoch 35/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1172 - val_loss: 2.0264\n",
            "Epoch 36/150\n",
            "22307/22307 [==============================] - 66s 3ms/step - loss: 2.1167 - val_loss: 2.0159\n",
            "Epoch 37/150\n",
            "22307/22307 [==============================] - 65s 3ms/step - loss: 2.1072 - val_loss: 2.0187\n",
            "Epoch 38/150\n",
            "22307/22307 [==============================] - 64s 3ms/step - loss: 2.0947 - val_loss: 2.0198\n",
            "Epoch 00038: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2ee6a9198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "VINKyXEYMIA2",
        "colab_type": "code",
        "outputId": "6e126644-e459-452c-b5a9-a0473df5947d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen, vocab_size), dtype=np.int32)\n",
        "    one_hot = [to_categorical(pattern[i], num_classes=vocab_size) for i in range(len(pattern))]\n",
        "    x = np.array(one_hot).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 0.7)\n",
        "    result = id2char[index]\n",
        "\n",
        "    seq_in = [id2char[value] for value in pattern]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" l\n",
            "would not infect his reason?\n",
            "\n",
            "ariel:\n",
            "n \"\n",
            "\n",
            "\n",
            "l\n",
            "would not infect his reason?\n",
            "\n",
            "ariel:\n",
            "not of he parises hin a wher a colly of fald\n",
            "of whe thourd he that that the yurisen\n",
            "he vere wound sone both mary breesios will in me me the e,\n",
            "you swing. hear eore as i puole sersiss of the sole!\n",
            "\n",
            "toarter:\n",
            "waver bith my wross our the beklen heand!\n",
            "thou the hean cotpore refered, to ghouch and baknel.\n",
            "\n",
            "lerfont:\n",
            "of not dutt the fronnsy thee lasy i scold.\n",
            "\n",
            "ungortoss:\n",
            "to it i ros wradtle heans i to shite whey,\n",
            "who dato cotterd poreo, and the froocs haar unkase\n",
            "then ment will  on ey mime;\n",
            "so beet af in patitel the seele.\n",
            "\n",
            "cotceus:\n",
            "shall fold the eret: set beait the goert:\n",
            "go the lence for siy wheor coce, and wios shify of heananld\n",
            "and miret shas sweil whye i the corngoun.\n",
            "\n",
            "foundon:\n",
            "the sould to sould is thou dose,\n",
            "thit soull hous gheet byel dore wete and,\n",
            "i with sith of not he my the here hear in the ourt.\n",
            "\n",
            "concincental:\n",
            "the badcing a mostely wond the rostle\n",
            "piten houlv andord slalt wher the riitent'd rach hice ateons\n",
            "lee and a in the nome, word, is wall you.\n",
            "\n",
            "kirket in:\n",
            "bes me houd; i dey of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dyIGZZgpLT7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mt_Wd4jaujI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 2"
      ]
    },
    {
      "metadata": {
        "id": "WlXEJN-MlQEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+1: i+maxlen+1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8IEuJIBlQKD",
        "colab_type": "code",
        "outputId": "bdf62549-b139-4f56-9684-ee3243a04601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9, 8, 31, 35, 18, 26, 4, 8, 18, 8, 36, 32, 5, 12, 24, 11, 32, 9, 25, 31, 32, 26, 2, 32, 26, 22, 31, 25, 4, 32, 32, 7, 26, 1, 5, 3, 26, 9, 34, 31] 40 27884\n",
            "[8, 31, 35, 18, 26, 4, 8, 18, 8, 36, 32, 5, 12, 24, 11, 32, 9, 25, 31, 32, 26, 2, 32, 26, 22, 31, 25, 4, 32, 32, 7, 26, 1, 5, 3, 26, 9, 34, 31, 18] 40 27884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1aKoxEFrlQNW",
        "colab_type": "code",
        "outputId": "e4f9a35a-3c65-4e34-aff3-1b2bd06f0289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', ''.join(id2char[i] for i in next_chars[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: first citizen:\n",
            "before we proceed any fur\n",
            "Output: irst citizen:\n",
            "before we proceed any furt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pII-4ULauIY-",
        "colab_type": "code",
        "outputId": "00bcb0d4-3d7f-4ec3-ede3-d34970ccb9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen, vocab_size), dtype=np.int32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(sentences)) for c in sentences[i]]\n",
        "X = np.array(one_hot).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]] (27884, 40, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e8rMrf4suYvC",
        "colab_type": "code",
        "outputId": "e208fcf1-6fe7-4b19-86f6-157c0ff72a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), maxlen), dtype=np.float32)\n",
        "Y = np.array(next_chars).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 8 31 35 18 26  4  8 18  8 36 32  5 12 24 11 32  9 25 31 32 26  2 32 26\n",
            " 22 31 25  4 32 32  7 26  1  5  3 26  9 34 31 18] (27884, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bumyf1qmJmGd",
        "colab_type": "code",
        "outputId": "744100fd-d3d1-4b20-d9cc-c8c639be5823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (22307, 40, 39) (22307, 40)\n",
            "Validation: (5577, 40, 39) (5577, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x73Ok2HduY07",
        "colab_type": "code",
        "outputId": "82f50b80-0f06-487a-8fa7-85b009634e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, vocab_size)))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 40, 512)           1130496   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 40, 39)            20007     \n",
            "=================================================================\n",
            "Total params: 3,249,703\n",
            "Trainable params: 3,249,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IsgyeYpHNuMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, vocab_size), dtype=np.int32)\n",
        "            one_hot = [to_categorical(char2id[c], num_classes=vocab_size) for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds[0], diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8gpYRKXkIge-",
        "colab_type": "code",
        "outputId": "da2aa4a9-2329-4b00-a303-e0da32ac8d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3782
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, np.expand_dims(train_y, -1),\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, np.expand_dims(val_y, -1)),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, np.expand_dims(train_y, -1),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, np.expand_dims(val_y, -1)),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 22307 samples, validate on 5577 samples\n",
            "Epoch 1/150\n",
            "22307/22307 [==============================] - 78s 4ms/step - loss: 2.7150 - val_loss: 2.2041\n",
            "Epoch 2/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 2.1877 - val_loss: 1.9637\n",
            "Epoch 3/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 2.0202 - val_loss: 1.8239\n",
            "Epoch 4/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.9141 - val_loss: 1.7318\n",
            "Epoch 5/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.8416 - val_loss: 1.6681\n",
            "Epoch 6/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.7888 - val_loss: 1.6246\n",
            "Epoch 7/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.7486 - val_loss: 1.5909\n",
            "Epoch 8/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.7171 - val_loss: 1.5626\n",
            "Epoch 9/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6905 - val_loss: 1.5415\n",
            "Epoch 10/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6698 - val_loss: 1.5262\n",
            "Epoch 11/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6508 - val_loss: 1.5125\n",
            "Epoch 12/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6342 - val_loss: 1.4995\n",
            "Epoch 13/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.6222 - val_loss: 1.4902\n",
            "Epoch 14/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.6098 - val_loss: 1.4803\n",
            "Epoch 15/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.5982 - val_loss: 1.4732\n",
            "Epoch 16/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.5887 - val_loss: 1.4632\n",
            "Epoch 17/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.5806 - val_loss: 1.4579\n",
            "Epoch 18/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5712 - val_loss: 1.4515\n",
            "Epoch 19/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5634 - val_loss: 1.4475\n",
            "Epoch 20/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5568 - val_loss: 1.4430\n",
            "Epoch 21/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5509 - val_loss: 1.4386\n",
            "Epoch 22/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5446 - val_loss: 1.4345\n",
            "Epoch 23/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5392 - val_loss: 1.4311\n",
            "Epoch 24/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5332 - val_loss: 1.4278\n",
            "Epoch 25/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5276 - val_loss: 1.4247\n",
            "Epoch 26/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5229 - val_loss: 1.4217\n",
            "Epoch 27/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5183 - val_loss: 1.4192\n",
            "Epoch 28/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5141 - val_loss: 1.4178\n",
            "Epoch 29/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5102 - val_loss: 1.4150\n",
            "Epoch 30/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5063 - val_loss: 1.4136\n",
            "Epoch 31/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5021 - val_loss: 1.4110\n",
            "Epoch 32/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.4989 - val_loss: 1.4089\n",
            "Epoch 33/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.4964 - val_loss: 1.4081\n",
            "Epoch 34/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.4920 - val_loss: 1.4049\n",
            "Epoch 35/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4891 - val_loss: 1.4031\n",
            "Epoch 36/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4857 - val_loss: 1.4017\n",
            "Epoch 37/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4829 - val_loss: 1.4008\n",
            "Epoch 38/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4792 - val_loss: 1.3998\n",
            "Epoch 39/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4773 - val_loss: 1.3985\n",
            "Epoch 40/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4742 - val_loss: 1.3987\n",
            "Epoch 41/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4722 - val_loss: 1.3965\n",
            "Epoch 42/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4693 - val_loss: 1.3963\n",
            "Epoch 43/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4672 - val_loss: 1.3958\n",
            "Epoch 44/150\n",
            " 9088/22307 [===========>..................] - ETA: 48s - loss: 1.4599Train on 22307 samples, validate on 5577 samples\n",
            "Epoch 1/150\n",
            "22307/22307 [==============================] - 78s 4ms/step - loss: 2.7150 - val_loss: 2.2041\n",
            "Epoch 2/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 2.1877 - val_loss: 1.9637\n",
            "Epoch 3/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 2.0202 - val_loss: 1.8239\n",
            "Epoch 4/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.9141 - val_loss: 1.7318\n",
            "Epoch 5/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.8416 - val_loss: 1.6681\n",
            "Epoch 6/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.7888 - val_loss: 1.6246\n",
            "Epoch 7/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.7486 - val_loss: 1.5909\n",
            "Epoch 8/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.7171 - val_loss: 1.5626\n",
            "Epoch 9/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6905 - val_loss: 1.5415\n",
            "Epoch 10/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6698 - val_loss: 1.5262\n",
            "Epoch 11/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6508 - val_loss: 1.5125\n",
            "Epoch 12/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.6342 - val_loss: 1.4995\n",
            "Epoch 13/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.6222 - val_loss: 1.4902\n",
            "Epoch 14/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.6098 - val_loss: 1.4803\n",
            "Epoch 15/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.5982 - val_loss: 1.4732\n",
            "Epoch 16/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.5887 - val_loss: 1.4632\n",
            "Epoch 17/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.5806 - val_loss: 1.4579\n",
            "Epoch 18/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5712 - val_loss: 1.4515\n",
            "Epoch 19/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5634 - val_loss: 1.4475\n",
            "Epoch 20/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5568 - val_loss: 1.4430\n",
            "Epoch 21/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5509 - val_loss: 1.4386\n",
            "Epoch 22/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5446 - val_loss: 1.4345\n",
            "Epoch 23/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5392 - val_loss: 1.4311\n",
            "Epoch 24/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5332 - val_loss: 1.4278\n",
            "Epoch 25/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5276 - val_loss: 1.4247\n",
            "Epoch 26/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5229 - val_loss: 1.4217\n",
            "Epoch 27/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5183 - val_loss: 1.4192\n",
            "Epoch 28/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5141 - val_loss: 1.4178\n",
            "Epoch 29/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5102 - val_loss: 1.4150\n",
            "Epoch 30/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5063 - val_loss: 1.4136\n",
            "Epoch 31/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.5021 - val_loss: 1.4110\n",
            "Epoch 32/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.4989 - val_loss: 1.4089\n",
            "Epoch 33/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.4964 - val_loss: 1.4081\n",
            "Epoch 34/150\n",
            "22307/22307 [==============================] - 77s 3ms/step - loss: 1.4920 - val_loss: 1.4049\n",
            "Epoch 35/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4891 - val_loss: 1.4031\n",
            "Epoch 36/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4857 - val_loss: 1.4017\n",
            "Epoch 37/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4829 - val_loss: 1.4008\n",
            "Epoch 38/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4792 - val_loss: 1.3998\n",
            "Epoch 39/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4773 - val_loss: 1.3985\n",
            "Epoch 40/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4742 - val_loss: 1.3987\n",
            "Epoch 41/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4722 - val_loss: 1.3965\n",
            "Epoch 42/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4693 - val_loss: 1.3963\n",
            "Epoch 43/150\n",
            "22307/22307 [==============================] - 76s 3ms/step - loss: 1.4672 - val_loss: 1.3958\n",
            "Epoch 44/150\n",
            "22307/22307 [==============================] - 82s 4ms/step - loss: 1.4647 - val_loss: 1.3950\n",
            "22307/22307 [==============================] - 82s 4ms/step - loss: 1.4647 - val_loss: 1.3950\n",
            "Epoch 45/150\n",
            "Epoch 45/150\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4618 - val_loss: 1.3915\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4618 - val_loss: 1.3915\n",
            "Epoch 46/150\n",
            "Epoch 46/150\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4596 - val_loss: 1.3932\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4596 - val_loss: 1.3932\n",
            "Epoch 47/150\n",
            "Epoch 47/150\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4560 - val_loss: 1.3909\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4560 - val_loss: 1.3909\n",
            "Epoch 48/150\n",
            "Epoch 48/150\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4556 - val_loss: 1.3899\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4556 - val_loss: 1.3899\n",
            "Epoch 49/150\n",
            "Epoch 49/150\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4529 - val_loss: 1.3915\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4529 - val_loss: 1.3915\n",
            "Epoch 50/150\n",
            "Epoch 50/150\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4514 - val_loss: 1.3900\n",
            "22307/22307 [==============================] - 78s 3ms/step - loss: 1.4514 - val_loss: 1.3900\n",
            "Epoch 00050: early stopping\n",
            "Epoch 00050: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe28851a748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe28851a748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "NFknAcM5hL_N",
        "colab_type": "code",
        "outputId": "fe2a0d9a-bc2b-4703-8acc-fbceea3440e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1009
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "#start = np.random.randint(0, len(sentences)-1)\n",
        "seed = \"to be, or not to be that is the question\"\n",
        "pattern = [char2id[value] for value in seed]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen, vocab_size), dtype=np.int32)\n",
        "    one_hot = [to_categorical(pattern[i], num_classes=vocab_size) for i in range(len(pattern))]\n",
        "    x = np.array(one_hot).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction[0], 0.8)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" to be, or not to be that is the question \"\n",
            "\n",
            "\n",
            "to be, or not to be that is the question  hu  wuimdrha mseythonhhdcaee\n",
            "su rponn:chiriyalfy,ueisrhtahendaeeotp,t ng sf  \n",
            "oesyder '  snce, e ina?t  , t hiirsr.eiwd e  r ecfieie\n",
            "\n",
            "mdhtgm\n",
            "usw\n",
            "hejisei,o rny cylhegnofsr dmie  yratn:a\n",
            "adon\n",
            "ac ffidsga;le'odaotes,ee\n",
            "\n",
            "l:rtig\n",
            "ltp'mcsleweoseo j\n",
            "l l  nuhdt rmato\n",
            " ofetd rivhti.arb artuwawitt,taeem\n",
            "etarwsduno gtte.on\n",
            "m rtlo t .eseo wmw ;eohbaoesnerpehen\n",
            "ndyou  ort.r\n",
            " t udaaht dn yp rhf haranwho. fthtomh\n",
            "e\n",
            "whtf rgmum tm.hpyiaya,op,oak\n",
            "fe'eo ,e\n",
            "pw?aaibdsi yihu\n",
            "ii'sl ! ol lnybelgrls .cie\n",
            "ysnl  nw .eradgs ac\n",
            "cwesag,ese\n",
            " o?y\n",
            "eerd?oduwoeiw\n",
            " gnie,mrluo lmrf   \n",
            "gls\n",
            "\n",
            "h,ne, woto .tinc.ssa i yruwoagrstyhul \n",
            "hi\n",
            "ca\n",
            "so i?f\n",
            "ltgr\n",
            ": yvlh ic cto  h  s,gptther\n",
            ",mmn\n",
            " jv ai\n",
            " t,ioartojeonoieso  oaooea t o dcnuofsmww. fldilues'\n",
            " tn fi\n",
            "rrnlawfhyrq agrfioaii\n",
            "me enasupspyhgwaf\n",
            "ea driio,uuf-rarsn'tsbidisot  opaoeeamacfmp aont\n",
            "ttl-s: eose.us bsp wntautricereoa wnr.ii roa \n",
            "n;oas\n",
            "s deeoteeorsh;lo  nuiiw a\n",
            "ntutorabd wrrtet  nledsu te\n",
            "snmginao bcbi rhltse gayohr nqodn elos t: i.eosoieaevt ilhonlrn re s uk oo,iltt \n",
            "tg\n",
            "elen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L39x6-iCEjYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8MglftqSurjO"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X-qtSEGQurjU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "b0ygkdZsurji",
        "outputId": "eb7d918f-116a-41a4-e038-a5f82e5ae5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9, 8, 31, 35, 18, 26, 4, 8, 18, 8, 36, 32, 5, 12, 24, 11, 32, 9, 25, 31, 32, 26, 2, 32, 26, 22, 31, 25, 4, 32, 32, 7, 26, 1, 5, 3, 26, 9, 34, 31] 40 27884\n",
            "[18] 1 27884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jCWnJx5vurj2",
        "outputId": "6e8c63ea-b95f-44fb-a9b3-2b0e5c06f230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: first citizen:\n",
            "before we proceed any fur\n",
            "Output: t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "naLncfiVurkJ",
        "outputId": "b9dab7d1-b813-42f9-ed1f-f7c68b531267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 9  8 31 35 18 26  4  8 18  8 36 32  5 12 24 11 32  9 25 31 32 26  2 32\n",
            " 26 22 31 25  4 32 32  7 26  1  5  3 26  9 34 31] (27884, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pXTmZ8csurke",
        "outputId": "fc62bc24-89b5-401f-b9c7-6bb0b46b8993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (27884, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsDXSEWdIx0V",
        "colab_type": "code",
        "outputId": "a3fcb59d-36ba-43f2-84d1-6a4a69734c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (22307, 40) (22307, 39)\n",
            "Validation: (5577, 40) (5577, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_yJRY7i_urkw",
        "outputId": "fcbc7b57-51fd-4276-e8a5-deef6df4d345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 40, 512)           19968     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 39)                20007     \n",
            "=================================================================\n",
            "Total params: 4,238,375\n",
            "Trainable params: 4,238,375\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "65RYvNzrurlK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p8A7kIBwurlU",
        "outputId": "af23c0aa-5716-4bcf-c75e-11d5fd2a0af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 22307 samples, validate on 5577 samples\n",
            "Epoch 1/150\n",
            "22307/22307 [==============================] - 87s 4ms/step - loss: 2.8657 - val_loss: 2.4483\n",
            "Epoch 2/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 2.4023 - val_loss: 2.2170\n",
            "Epoch 3/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 2.2510 - val_loss: 2.1134\n",
            "Epoch 4/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 2.1455 - val_loss: 2.0431\n",
            "Epoch 5/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 2.0635 - val_loss: 1.9687\n",
            "Epoch 6/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 1.9878 - val_loss: 1.9257\n",
            "Epoch 7/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 1.9303 - val_loss: 1.8912\n",
            "Epoch 8/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 1.8712 - val_loss: 1.8708\n",
            "Epoch 9/150\n",
            "22307/22307 [==============================] - 86s 4ms/step - loss: 1.8208 - val_loss: 1.8609\n",
            "Epoch 10/150\n",
            "22307/22307 [==============================] - 85s 4ms/step - loss: 1.7728 - val_loss: 1.8373\n",
            "Epoch 11/150\n",
            "22307/22307 [==============================] - 85s 4ms/step - loss: 1.7283 - val_loss: 1.8485\n",
            "Epoch 12/150\n",
            "22307/22307 [==============================] - 86s 4ms/step - loss: 1.6907 - val_loss: 1.8295\n",
            "Epoch 13/150\n",
            "22307/22307 [==============================] - 85s 4ms/step - loss: 1.6463 - val_loss: 1.8534\n",
            "Epoch 14/150\n",
            "22307/22307 [==============================] - 84s 4ms/step - loss: 1.6025 - val_loss: 1.8450\n",
            "Epoch 00014: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2a32c73c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ecSJiOp4urli",
        "outputId": "ca902973-5039-4b62-84dc-84de65f69551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" twas i won the wager, though you hit the \"\n",
            "\n",
            "\n",
            "twas i won the wager, though you hit theirs frate him, for!\n",
            "\n",
            " carmily:\n",
            "he somine to not nobl a lome cray dauwer,\n",
            "and i could soun would well remate thrank les\n",
            "us poarment frowseld fair us, to cently pracael;\n",
            "and iis\n",
            "weng the beapteh guing\n",
            "fir cloices, for stance worch you o'n leak.\n",
            "\n",
            "lord,\n",
            "wwith o't gouw.\n",
            "\n",
            "dostcnirius:\n",
            "o come, to baureful pey, my forth thou wing for to die,-\n",
            "chrempitelsoo!\n",
            "so me, stend hard confixter me willion gaord?\n",
            "\n",
            "cloucontily:\n",
            "ay, to thou epetulimer\n",
            "of otding of encesten.\n",
            "\n",
            "-stiben:\n",
            "i geid, swat, seo; ride, i gray gon my this got's dance!\n",
            "is cuse remolgle, and sour?\n",
            "\n",
            "your: i doach of barcaintul, blouch\n",
            "hating of jessulce tre! you thear not!\n",
            "thing but you ruse claopol a cunkcep;\n",
            "you as not his math'r wife.\n",
            "but minusty of hath firtof of swand to of deagh.\n",
            "\n",
            "vesjuncenio:\n",
            "cive my hould hivenore, with that go liccired?\n",
            " dever\n",
            "ole caore of in\n",
            "this of secsen pricools would pinty know!\n",
            "\n",
            "dost that sweet youch\n",
            "that dequcon hid ummin you.\n",
            "\n",
            "firsst remerbar:\n",
            "they giarch you houke, whal apip me, but seit,\n",
            "then sceact en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7SLDfWM3urls",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRANg_pH11-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Nietzsche RNN"
      ]
    },
    {
      "metadata": {
        "id": "ZGsyW71213eq",
        "colab_type": "code",
        "outputId": "d2061ced-efd5-4c75-cd99-177ff3fafb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/nietzsche.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "preface\n",
            "\n",
            "\n",
            "supposing that truth is a woman--what then? is there not ground\n",
            "for suspecting that all philosophers, in so far as they have been\n",
            "dogmatists, have failed to understand women--that the terrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6uA4vpm2JXG",
        "colab_type": "code",
        "outputId": "53a67725-5af6-437b-8c87-fa0a2ebc4c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'l', '8', 'a', '5', '=', 'w', 'y', 'æ', 'ë', '0', 'c', 'n', '_', '2', '(', 'q', 'd', 'i', 'f', ',', 'h', 'b', ':', 'v', '7', 'k', 'x', 't', '.', '1', ']', '?', ';', 'p', 'g', '\\n', 'o', ' ', '!', 'j', '3', '4', 'r', 'e', '\"', 'm', 'u', 'é', 's', 'z', '[', '-', '9', 'ä', ')', \"'\", '6'}\n",
            "Length of Unique characters: 57\n",
            "Number of characters in data: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wE3jqpwK2JQO",
        "colab_type": "code",
        "outputId": "bb7857a0-31ae-4051-fa22-f1f943d4f1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'l': 0, '8': 1, 'a': 2, '5': 3, '=': 4, 'w': 5, 'y': 6, 'æ': 7, 'ë': 8, '0': 9, 'c': 10, 'n': 11, '_': 12, '2': 13, '(': 14, 'q': 15, 'd': 16, 'i': 17, 'f': 18, ',': 19, 'h': 20, 'b': 21, ':': 22, 'v': 23, '7': 24, 'k': 25, 'x': 26, 't': 27, '.': 28, '1': 29, ']': 30, '?': 31, ';': 32, 'p': 33, 'g': 34, '\\n': 35, 'o': 36, ' ': 37, '!': 38, 'j': 39, '3': 40, '4': 41, 'r': 42, 'e': 43, '\"': 44, 'm': 45, 'u': 46, 'é': 47, 's': 48, 'z': 49, '[': 50, '-': 51, '9': 52, 'ä': 53, ')': 54, \"'\": 55, '6': 56}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4fXnyG-52A4u",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FqM3VBxo2A5B",
        "outputId": "d10c9374-2dc9-4da9-cc16-8eb6084c16f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[33, 42, 43, 18, 2, 10, 43, 35, 35, 35, 48, 46, 33, 33, 36, 48, 17, 11, 34, 37, 27, 20, 2, 27, 37, 27, 42, 46, 27, 20, 37, 17, 48, 37, 2, 37, 5, 36, 45, 2] 40 15022\n",
            "[11] 1 15022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DOfFs7K-2A5V",
        "outputId": "14b9962e-aa68-4141-a319-cdf85ee00f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: preface\n",
            "\n",
            "\n",
            "supposing that truth is a woma\n",
            "Output: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Lg7giesN2A5k",
        "outputId": "5930a406-1108-4d01-be7c-8efbe842d09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[33 42 43 18  2 10 43 35 35 35 48 46 33 33 36 48 17 11 34 37 27 20  2 27\n",
            " 37 27 42 46 27 20 37 17 48 37  2 37  5 36 45  2] (15022, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FMwOzY_02A5v",
        "outputId": "ab99d35e-6714-4c9d-c0ea-0d81b2538fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.] (15022, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xcTTZ4UC2A58",
        "outputId": "548e5d34-0630-43bf-eb36-49338596fa8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (12017, 40) (12017, 57)\n",
            "Validation: (3005, 40) (3005, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8u1pT53O2A6L",
        "outputId": "643d3995-4789-4fd5-f60a-985c95a5992f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 40, 512)           29184     \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 57)                29241     \n",
            "=================================================================\n",
            "Total params: 4,256,825\n",
            "Trainable params: 4,256,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wWQ9Btlr2A6i",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i2_J8Up_2A6y",
        "outputId": "e1d52661-d6cd-416a-a57b-acfcbc16d9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12017 samples, validate on 3005 samples\n",
            "Epoch 1/150\n",
            "12017/12017 [==============================] - 48s 4ms/step - loss: 3.1232 - val_loss: 2.7923\n",
            "Epoch 2/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 2.6548 - val_loss: 2.4334\n",
            "Epoch 3/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 2.4478 - val_loss: 2.2918\n",
            "Epoch 4/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 2.3304 - val_loss: 2.2006\n",
            "Epoch 5/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 2.2395 - val_loss: 2.1407\n",
            "Epoch 6/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 2.1571 - val_loss: 2.0891\n",
            "Epoch 7/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 2.0743 - val_loss: 2.0540\n",
            "Epoch 8/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 1.9985 - val_loss: 2.0355\n",
            "Epoch 9/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 1.9310 - val_loss: 2.0051\n",
            "Epoch 10/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 1.8501 - val_loss: 2.0133\n",
            "Epoch 11/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 1.7831 - val_loss: 2.0012\n",
            "Epoch 12/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 1.7061 - val_loss: 2.0276\n",
            "Epoch 13/150\n",
            "12017/12017 [==============================] - 45s 4ms/step - loss: 1.6474 - val_loss: 2.0343\n",
            "Epoch 00013: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2a51abc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uQ_U1iE62A6_",
        "outputId": "0ac0ec96-aa4d-42f5-faf9-d55040934610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" ibility: as is whatever rests upon the e \"\n",
            "\n",
            "\n",
            "ibility: as is whatever rests upon the ertcatk; the swrow high\n",
            "stryst (nproully on indinghen, a\n",
            "weenf thit a nofnermant\n",
            "the shand bedoung, beoore\n",
            "maty\n",
            "vesy mist\n",
            "mand'r masiding;--wa chover now be. prane that to a cistenfty that\n",
            "with ont\n",
            "lon rawes- is a aues it prectore indeidend (msot mast, the mowely, and sond il siit to too this ,priveror\n",
            "\"bqurauns to dode 4ed whould svein\n",
            "3f man spouctaser, sentikne, as we man whomen wo  so lingint, thal mist 9vey as presalsize\n",
            "in a corteltint in\n",
            "ponoftunle-, in the reatser bey re hiss soplesos, thit whead nomebses\n",
            "beclectian intorpothe or sother latuse--wall momelhess e\n",
            "exizandi\"t and maks cunligion wikh the sover\n",
            "tith themes but be tend,\n",
            "at\n",
            "paswsicats thear\n",
            "sour ypess\"9hind fa, and the ermine\n",
            "hat wain hich this dawter of pradsiens, indelfencetasy, (ron the ertanner\n",
            "horging, concu(haquariom and ab\n",
            "egear\"y, bees hebeprecever and the cotranss of them devery. in the bokverely thet-with\n",
            "domeporing ther, inkoly\n",
            "pirophisior, and that the, tood. fol and hedaro:, ak leity semcant p7araovic,\n",
            "-isc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8oLpFOI-2A7i",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GyPcaBfz2nom"
      },
      "cell_type": "markdown",
      "source": [
        "## AustenRNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Si4qo-aJ2nos",
        "outputId": "b60c7d79-68c1-45c9-87dc-5a7e21b16dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/31100.txt'\n",
        "import io\n",
        "\n",
        "with io.open(file_path, 'r', encoding='windows-1252') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "\n",
            "project gutenberg's the complete works of jane austen, by jane austen\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  you may copy it, give it aw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uufi9SGA2no6",
        "outputId": "ce46c470-8ed1-4cb0-91dd-6ae02cfc26fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)\n",
        "\n",
        "#restricting so that we can load all data onto RAM\n",
        "#uncomment this line to run on all data\n",
        "data = data[:473597]\n",
        "data_size = len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'l', '5', 'a', '8', 'w', 'y', '0', 'c', 'n', '_', '2', '(', 'q', 'd', ',', 'f', 'i', 'b', 'h', ':', 'v', '#', '7', '&', 'k', ']', 't', '.', '1', 'x', '?', ';', '/', 'p', 'g', 'o', '\\n', ' ', '!', 'j', '3', '$', '4', 'r', 'e', '*', 'm', 'u', '\"', '%', 's', 'z', '[', '-', '9', '\\xa0', ')', '@', \"'\", '6'}\n",
            "Length of Unique characters: 60\n",
            "Number of characters in data: 4373597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zAKnX3872npF",
        "outputId": "bca1b59d-e08b-4950-f515-adcb206be8f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'l': 0, '5': 1, 'a': 2, '8': 3, 'w': 4, 'y': 5, '0': 6, 'c': 7, 'n': 8, '_': 9, '2': 10, '(': 11, 'q': 12, 'd': 13, ',': 14, 'f': 15, 'i': 16, 'b': 17, 'h': 18, ':': 19, 'v': 20, '#': 21, '7': 22, '&': 23, 'k': 24, ']': 25, 't': 26, '.': 27, '1': 28, 'x': 29, '?': 30, ';': 31, '/': 32, 'p': 33, 'g': 34, 'o': 35, '\\n': 36, ' ': 37, '!': 38, 'j': 39, '3': 40, '$': 41, '4': 42, 'r': 43, 'e': 44, '*': 45, 'm': 46, 'u': 47, '\"': 48, '%': 49, 's': 50, 'z': 51, '[': 52, '-': 53, '9': 54, '\\xa0': 55, ')': 56, '@': 57, \"'\": 58, '6': 59}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qMyByfwb2npQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9cbTWA9_2npY",
        "outputId": "253ea670-ddc3-4a24-f38d-0e496a89cbb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36, 33, 43, 35, 39, 44, 7, 26, 37, 34, 47, 26, 44, 8, 17, 44, 43, 34, 58, 50, 37, 26, 18, 44, 37, 7, 35, 46, 33, 0, 44, 26, 44, 37, 4, 35, 43, 24, 50, 37] 40 11839\n",
            "[35] 1 11839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "85s9OzRV2npn",
        "outputId": "f3819491-38d0-4742-ddca-0445a932c947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "project gutenberg's the complete works \n",
            "Output: o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q66s-_jz2np2",
        "outputId": "87acd930-68f2-45bb-e22d-d48a724c437e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36 33 43 35 39 44  7 26 37 34 47 26 44  8 17 44 43 34 58 50 37 26 18 44\n",
            " 37  7 35 46 33  0 44 26 44 37  4 35 43 24 50 37] (11839, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P-5hwjKq2nqH",
        "outputId": "fbb9159d-6d29-4c8b-e656-f9444ac1aed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (11839, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9Tcnlc7q2nqh",
        "outputId": "32886172-9f88-4284-e470-39d75d58c7d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (9471, 40) (9471, 60)\n",
            "Validation: (2368, 40) (2368, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "j55YI6ba2nqu",
        "outputId": "7e7b3da7-4564-449e-9646-3d67989a0688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 40, 512)           30720     \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 60)                30780     \n",
            "=================================================================\n",
            "Total params: 4,259,900\n",
            "Trainable params: 4,259,900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dJIM4A3A2nq4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nPTSRu6Y2nrN",
        "outputId": "ae080e53-d0f0-4d26-dddf-fd75a82cd1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9471 samples, validate on 2368 samples\n",
            "Epoch 1/150\n",
            "9471/9471 [==============================] - 39s 4ms/step - loss: 3.1477 - val_loss: 2.9065\n",
            "Epoch 2/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 2.7838 - val_loss: 2.5611\n",
            "Epoch 3/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 2.5375 - val_loss: 2.3823\n",
            "Epoch 4/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 2.3890 - val_loss: 2.2715\n",
            "Epoch 5/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 2.2753 - val_loss: 2.2101\n",
            "Epoch 6/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 2.1978 - val_loss: 2.1443\n",
            "Epoch 7/150\n",
            "9471/9471 [==============================] - 35s 4ms/step - loss: 2.1120 - val_loss: 2.1009\n",
            "Epoch 8/150\n",
            "9471/9471 [==============================] - 35s 4ms/step - loss: 2.0308 - val_loss: 2.0594\n",
            "Epoch 9/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 1.9458 - val_loss: 2.0432\n",
            "Epoch 10/150\n",
            "9471/9471 [==============================] - 37s 4ms/step - loss: 1.8641 - val_loss: 2.0200\n",
            "Epoch 11/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 1.7884 - val_loss: 2.0387\n",
            "Epoch 12/150\n",
            "9471/9471 [==============================] - 36s 4ms/step - loss: 1.7135 - val_loss: 2.0279\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2a3342be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tAkW-BYS2nre",
        "outputId": "34bb20d0-c135-4bb4-88c4-f0f8826e8c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" cut of a laundress and a waiter,\n",
            "rather  \"\n",
            "\n",
            "\n",
            "cut of a laundress and a waiter,\n",
            "rather mres anthoingh; nom?\"  ho cupsan a\n",
            "recuck: \n",
            "aspich the, willly; as hom soene the\n",
            "orpteling ibgist hersecof. so, and and thet the ere helcieh new at and thonk\n",
            "sthe\n",
            "read and in the\n",
            "heond had satrexmf; shertyencwoth, that levifile, with band in sorird,\n",
            "ylosed anne oefores had main by, dot soger\n",
            "sot lome; whish., a\n",
            "cuptes chomk hishirlither, their. \n",
            "thoughein, gir he. \n",
            "acpants, or whe .n could to\n",
            "mirk, of him, ablavant'.\n",
            " the ald enkitally, and doed\n",
            "the polgont not)th, of cevlicent lo\"dsint! bots shiwh tho egren?ss\n",
            "on her mapes no\n",
            "stittling seaver heroane, pumketter sorther ryxkess-yexssoore of, for forther's.\n",
            ".\n",
            "\n",
            "sokover rigbss, ind bath and the mrettrill akviceirt in scentoneacapain.  i eonevill, for withs, sher' chingifary soy whith me elcestencmicujiwicion, engibe ristt muth; at a rons, than wall tell wanted wankereing comfolectan im bittle; and of elliedst, and she was kee sare a bevor beed mistwelted, haidsedcy hid chark wes she o\n",
            "ynevist mild.  she the sind ant\n",
            "caveos whound, the evu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GucIVO_52nro",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H_HjwiJ_3Mia"
      },
      "cell_type": "markdown",
      "source": [
        "## DictionaryRNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8eBrFhkQ3Mil",
        "outputId": "8e133c2d-c9e6-44bf-840f-f087af4ec49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/pg29765.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "﻿the project gutenberg ebook of webster's unabridged dictionary, by various\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  you may copy it, give \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qkYI0H943Mix",
        "outputId": "ec064ed3-5fa8-48b0-a6f3-a94330023d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)\n",
        "\n",
        "#restricting so that we can load all data onto RAM\n",
        "#uncomment this line to run on all data\n",
        "data = data[:156206]\n",
        "data_size = len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'ð', 'l', '5', '8', '=', 'û', '2', 'd', 'á', 'b', '#', 'v', '<', 't', ';', '\\n', 'ç', '½', '}', '\\\\', 'â', 'e', 'u', '>', 's', 'ù', 'ò', 'ö', '[', '9', '6', '¿', '¼', 'y', 'c', ',', 'f', 'h', ':', '1', 'ê', '^', 'o', '!', 'j', 'ú', 'r', 'ô', 'é', '\"', '%', '`', 'è', '\\t', '-', '\\ufeff', 'î', '@', 'ä', 'ë', 'w', '£', '_', '(', '/', 'i', '7', 'x', '.', 'þ', '§', '×', 'í', '3', '4', 'º', 'ì', '~', '°', 'z', 'ñ', 'ó', '+', '¾', ')', '{', '|', '÷', 'a', 'æ', 'ï', '0', 'n', 'ý', 'q', '&', 'k', ']', 'p', 'g', 'à', ' ', '$', '*', 'ã', 'm', 'å', 'ü', \"'\"}\n",
            "Length of Unique characters: 109\n",
            "Number of characters in data: 27956206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EiXOzjGX3Mi8",
        "outputId": "3e04eb30-b4e8-416c-bc3a-9863d6a4e3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'ð': 0, 'l': 1, '5': 2, '8': 3, '=': 4, 'û': 5, '2': 6, 'd': 7, 'á': 8, 'b': 9, '#': 10, 'v': 11, '<': 12, 't': 13, ';': 14, '\\n': 15, 'ç': 16, '½': 17, '}': 18, '\\\\': 19, 'â': 20, 'e': 21, 'u': 22, '>': 23, 's': 24, 'ù': 25, 'ò': 26, 'ö': 27, '[': 28, '9': 29, '6': 30, '¿': 31, '¼': 32, 'y': 33, 'c': 34, ',': 35, 'f': 36, 'h': 37, ':': 38, '1': 39, 'ê': 40, '^': 41, 'o': 42, '!': 43, 'j': 44, 'ú': 45, 'r': 46, 'ô': 47, 'é': 48, '\"': 49, '%': 50, '`': 51, 'è': 52, '\\t': 53, '-': 54, '\\ufeff': 55, 'î': 56, '@': 57, 'ä': 58, 'ë': 59, 'w': 60, '£': 61, '_': 62, '(': 63, '/': 64, 'i': 65, '7': 66, 'x': 67, '.': 68, 'þ': 69, '§': 70, '×': 71, 'í': 72, '3': 73, '4': 74, 'º': 75, 'ì': 76, '~': 77, '°': 78, 'z': 79, 'ñ': 80, 'ó': 81, '+': 82, '¾': 83, ')': 84, '{': 85, '|': 86, '÷': 87, 'a': 88, 'æ': 89, 'ï': 90, '0': 91, 'n': 92, 'ý': 93, 'q': 94, '&': 95, 'k': 96, ']': 97, 'p': 98, 'g': 99, 'à': 100, ' ': 101, '$': 102, '*': 103, 'ã': 104, 'm': 105, 'å': 106, 'ü': 107, \"'\": 108}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yiA0wFrG3MjF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eQpeCO4M3MjR",
        "outputId": "dcbe2306-c76b-42ac-e0c8-a40497f6592e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[55, 13, 37, 21, 101, 98, 46, 42, 44, 21, 34, 13, 101, 99, 22, 13, 21, 92, 9, 21, 46, 99, 101, 21, 9, 42, 42, 96, 101, 42, 36, 101, 60, 21, 9, 24, 13, 21, 46, 108] 40 3905\n",
            "[24] 1 3905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0yu_XY0I3Mjf",
        "outputId": "2559e8e4-b15e-44f1-cb2f-91a8dde0e19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: ﻿the project gutenberg ebook of webster'\n",
            "Output: s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vjYOqRj33Mjr",
        "outputId": "984f9d4e-1596-4c58-a9a4-bf1eca5b42a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 55  13  37  21 101  98  46  42  44  21  34  13 101  99  22  13  21  92\n",
            "   9  21  46  99 101  21   9  42  42  96 101  42  36 101  60  21   9  24\n",
            "  13  21  46 108] (3905, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fombJupP3Mj9",
        "outputId": "ceb73974-196a-44f4-b9da-4f5278e13f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (3905, 109)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lUk9uvxf3MkP",
        "outputId": "16a0bfa7-1791-4fb2-89f5-d221b498aa24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (3124, 40) (3124, 109)\n",
            "Validation: (781, 40) (781, 109)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U8D0lwQQ3Mkm",
        "outputId": "117fea8e-98f9-4ba8-c8a0-c61e86f2e48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 40, 512)           55808     \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 109)               55917     \n",
            "=================================================================\n",
            "Total params: 4,310,125\n",
            "Trainable params: 4,310,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lBqbYPKk3Mku",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "u2ztL81o3Mk2",
        "outputId": "26c81e7e-8bfa-4300-ac17-de0111d98459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3124 samples, validate on 781 samples\n",
            "Epoch 1/150\n",
            "3124/3124 [==============================] - 16s 5ms/step - loss: 3.5881 - val_loss: 3.1845\n",
            "Epoch 2/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 3.3307 - val_loss: 3.1500\n",
            "Epoch 3/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 3.2715 - val_loss: 3.1223\n",
            "Epoch 4/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 3.1907 - val_loss: 2.9859\n",
            "Epoch 5/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 3.0478 - val_loss: 2.8291\n",
            "Epoch 6/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.8933 - val_loss: 2.6635\n",
            "Epoch 7/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.7643 - val_loss: 2.5323\n",
            "Epoch 8/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.6234 - val_loss: 2.4410\n",
            "Epoch 9/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.5247 - val_loss: 2.4051\n",
            "Epoch 10/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.4396 - val_loss: 2.3445\n",
            "Epoch 11/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.3454 - val_loss: 2.2985\n",
            "Epoch 12/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.2680 - val_loss: 2.2855\n",
            "Epoch 13/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.1819 - val_loss: 2.2910\n",
            "Epoch 14/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 2.0962 - val_loss: 2.2500\n",
            "Epoch 15/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 1.9760 - val_loss: 2.3141\n",
            "Epoch 16/150\n",
            "3124/3124 [==============================] - 12s 4ms/step - loss: 1.9135 - val_loss: 2.3388\n",
            "Epoch 00016: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2a10f3668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "twmeclnI3MlA",
        "outputId": "3928d545-cce9-48b1-e49e-9d0f86b21ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" lled an absent man is commonly either a  \"\n",
            "\n",
            "\n",
            "lled an absent man is commonly either a to fre pased wonn.\n",
            "\n",
            "2. thintopyp an, the sindeleded\n",
            "yre ale pee) the absotel; horersava, sethe\n",
            "torihe\n",
            "bxepaa sepennened.\n",
            "\n",
            "defn: sele it a obtinh, frilont abnm the hensed; enertsewitk fong and a bausting; [tamddverein; nenthe cidtichepubu#\n",
            "a[us.ir, d.t[.\n",
            "[pyfn: mhiale il ofmeos oaptefss\n",
            "verterelunion: is the abtia abalelinoracishe whe qish*in to soubliirhel to sheae a abbingler]\n",
            "pefn: othe qretiamhs them loule ihes dety or a lende.\n",
            "\n",
            "an(linefand l.]\n",
            "\n",
            "d.f:\n",
            "\n",
            "he fuf sorion ¼en) main; te hrafptiseaune\n",
            " in ug-disg syer\n",
            "sectre to fef. the uwtasine\n",
            "lale or hrypbsgebthe ne te'nd ormaube, lle..\n",
            "\n",
            "bebtitilely\n",
            "aboc. i*ctrac*teins, n.\n",
            "\n",
            "defn: a ne the\n",
            " im vatyg\n",
            "\"raic) of sikhipe patite\n",
            "ley [cowsrade ;re. opl once mtonm, or al, the ubbite\n",
            "the wisilentia whoruey\n",
            "asirite falirgl\n",
            "ya me ore leenon) iffdiwde\n",
            "\n",
            "hes(a, hyne.\n",
            " eng. alerocery*tes (grosst latyl\n",
            "oy here.\n",
            "\n",
            "\"ed unasg, accomding the-; methisrile sipingdiil istyng to po the hale cobin noncir austded hiknq\n",
            "ticichys paocidicortigse f whithichim, oupcung\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SqaYeUgE3MlI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bSOwZneR5DJr"
      },
      "cell_type": "markdown",
      "source": [
        "## ObamaRNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qaNCvBcH5DJ3",
        "outputId": "cdd87aed-730a-4603-afb3-9505981714bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/obama/input.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "to chip, kathy, and nancy, who graciously shared your father with a nation that loved him; to walter's friends, colleagues, protégés, and all who considered him a hero; to the men of the intrepid; to \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WDzZo8xE5DKF",
        "outputId": "3c692f2b-2a8a-4093-cf8c-7f3cb8042e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)\n",
        "\n",
        "#restricting so that we can load all data onto RAM\n",
        "#uncomment this line to run on all data\n",
        "data = data[:256206]\n",
        "data_size = len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'l', '8', '5', '—', '2', 'd', 'á', 'b', 'v', '<', 't', '?', ';', '\\n', 'ַ', 'ç', '\\x92', 'e', 'u', '’', '>', 's', '[', '9', 'ב', '6', 'ר', '¼', 'y', '²', 'c', ',', 'f', 'h', ':', '1', 'o', '!', 'j', 'ą', '“', 'r', 'ô', 'é', '\"', '%', '`', 'è', 'ֹ', '-', '”', 'w', '(', '/', 'i', '7', 'x', '.', '–', 'í', '3', '4', 'z', 'ł', 'ñ', 'ó', 'ּ', '+', ')', '¹', '‘', 'ָ', 'ę', 'a', 'ï', '0', 'n', 'q', '&', 'k', ']', 'ת', '…', 'p', 'g', 'à', ' ', 'ו', 'ה', 'ד', '$', '*', 'm', \"'\"}\n",
            "Length of Unique characters: 94\n",
            "Number of characters in data: 4224143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7Nxnb1V55DKX",
        "outputId": "0f30e842-7f08-4fb7-9af9-6de6c3976aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'l': 0, '8': 1, '5': 2, '—': 3, '2': 4, 'd': 5, 'á': 6, 'b': 7, 'v': 8, '<': 9, 't': 10, '?': 11, ';': 12, '\\n': 13, 'ַ': 14, 'ç': 15, '\\x92': 16, 'e': 17, 'u': 18, '’': 19, '>': 20, 's': 21, '[': 22, '9': 23, 'ב': 24, '6': 25, 'ר': 26, '¼': 27, 'y': 28, '²': 29, 'c': 30, ',': 31, 'f': 32, 'h': 33, ':': 34, '1': 35, 'o': 36, '!': 37, 'j': 38, 'ą': 39, '“': 40, 'r': 41, 'ô': 42, 'é': 43, '\"': 44, '%': 45, '`': 46, 'è': 47, 'ֹ': 48, '-': 49, '”': 50, 'w': 51, '(': 52, '/': 53, 'i': 54, '7': 55, 'x': 56, '.': 57, '–': 58, 'í': 59, '3': 60, '4': 61, 'z': 62, 'ł': 63, 'ñ': 64, 'ó': 65, 'ּ': 66, '+': 67, ')': 68, '¹': 69, '‘': 70, 'ָ': 71, 'ę': 72, 'a': 73, 'ï': 74, '0': 75, 'n': 76, 'q': 77, '&': 78, 'k': 79, ']': 80, 'ת': 81, '…': 82, 'p': 83, 'g': 84, 'à': 85, ' ': 86, 'ו': 87, 'ה': 88, 'ד': 89, '$': 90, '*': 91, 'm': 92, \"'\": 93}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A_VgsNUn5DKi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hmvHHlwI5DKu",
        "outputId": "d7038ecb-8cae-4a13-91ae-1bb20ef5b999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10, 36, 86, 30, 33, 54, 83, 31, 86, 79, 73, 10, 33, 28, 31, 86, 73, 76, 5, 86, 76, 73, 76, 30, 28, 31, 86, 51, 33, 36, 86, 84, 41, 73, 30, 54, 36, 18, 21, 0] 40 6405\n",
            "[28] 1 6405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MNGnxgA85DK8",
        "outputId": "be26f910-1ecf-4a61-f7da-369c63e00d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: to chip, kathy, and nancy, who graciousl\n",
            "Output: y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1JHPocWd5DLJ",
        "outputId": "cd0c9e92-2809-44a5-8892-6b985458e35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10 36 86 30 33 54 83 31 86 79 73 10 33 28 31 86 73 76  5 86 76 73 76 30\n",
            " 28 31 86 51 33 36 86 84 41 73 30 54 36 18 21  0] (6405, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FY15OP4h5DLZ",
        "outputId": "ef15ed82-0cc5-4ec8-ba4d-ec7de32cb863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (6405, 94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1Ugn6de75DLw",
        "outputId": "21214eff-a977-4a93-f722-0e93431f1182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (5124, 40) (5124, 94)\n",
            "Validation: (1281, 40) (1281, 94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cDtZpSEK5DML",
        "outputId": "e57e64d2-36dd-4c66-dfa3-24867abf85b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 40, 512)           48128     \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 94)                48222     \n",
            "=================================================================\n",
            "Total params: 4,294,750\n",
            "Trainable params: 4,294,750\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MPIywPxE5DMa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aRMqATsy5DMl",
        "outputId": "8beaf46f-f061-4316-b422-0eb9257db760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5124 samples, validate on 1281 samples\n",
            "Epoch 1/150\n",
            "5124/5124 [==============================] - 24s 5ms/step - loss: 3.2475 - val_loss: 2.9458\n",
            "Epoch 2/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 3.1078 - val_loss: 2.8724\n",
            "Epoch 3/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.9030 - val_loss: 2.7373\n",
            "Epoch 4/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.7901 - val_loss: 2.6270\n",
            "Epoch 5/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.6453 - val_loss: 2.4944\n",
            "Epoch 6/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.5347 - val_loss: 2.4377\n",
            "Epoch 7/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.4566 - val_loss: 2.3797\n",
            "Epoch 8/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.3906 - val_loss: 2.3483\n",
            "Epoch 9/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.3232 - val_loss: 2.3380\n",
            "Epoch 10/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.2680 - val_loss: 2.2976\n",
            "Epoch 11/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.1987 - val_loss: 2.2674\n",
            "Epoch 12/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.1472 - val_loss: 2.2567\n",
            "Epoch 13/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 2.0742 - val_loss: 2.2466\n",
            "Epoch 14/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 1.9857 - val_loss: 2.2578\n",
            "Epoch 15/150\n",
            "5124/5124 [==============================] - 20s 4ms/step - loss: 1.9268 - val_loss: 2.2838\n",
            "Epoch 00015: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe28e7ecc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DxIHjO_45DM3",
        "outputId": "de253b26-b4a4-4a30-e45f-67e43ef7539c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" d vast amounts of money on things that a \"\n",
            "\n",
            "\n",
            "d vast amounts of money on things that athat't of losef.\n",
            "\n",
            "boit we ,dod weunn rantifil a goond nith ade anva'gar toleed bat y-iwes you chesumen five froopen mabkonilg o5 offernrectenss by proteupic chagent. nriog a inacpoes thive exsou seary comlid and e no0\n",
            "this hire lboar. of whane. ttring i hot doto whrore and hecsiins., comtels wiin to ranfte ams iñhaobels bomnfoy, aad ind ficy ;ouute kfut wer feoune venmy. n\n",
            "wo whew joit hey eceolle mecpolniwih tres end i fuver..\n",
            " -- thenedi dhxotund freite,trand whal golld wenph at i bethd. fuv the dingicaing o't whith aring theas nas the we, pomst.rothhes, seterilissy of what  et rmoslen, beviwass everii¹ lecridilg or thesrededed, aadd ifmilgagien pripalel but.s\n",
            "t'utmhed wrich wo proat i)s yurmitins bate., wevennd yoow riyin, that silet as umabawiut in yreiengis of thiretja serpepbur..\n",
            "\n",
            "ew wre2s e1ldowat  retralmcpith a haase.,\n",
            "rewums or dlompsein woars, ond thin'ns sirtign ald wunf ald dans tho efonl and yrevenal. in ,iwk necbes mamalicin horid, cresemecere ebow atme, ugfuwaure in wee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wf3oqm6h5DM8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BH5-AvvO3hZs"
      },
      "cell_type": "markdown",
      "source": [
        "## TrumpRNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lEBsKRuO3hZ5",
        "outputId": "b97083e3-fd8e-43c4-ab21-c11731ab3226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/speeches.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "﻿speech 1\n",
            "\n",
            "\n",
            "...thank you so much.  that's so nice.  isn't he a great guy.  he doesn't get a fair press; he doesn't get it.  it's just not fair.  and i have to tell you i'm here, and very strongly here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vk1xhVXy3haM",
        "outputId": "1f644559-b735-45ab-8f6f-3175320d41ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)\n",
        "\n",
        "#restricting so that we can load all data onto RAM\n",
        "#uncomment this line to run on all data\n",
        "data = data[:496270]\n",
        "data_size = len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'l', '5', 'a', '8', '=', 'w', 'y', '0', 'c', 'n', '—', '2', '_', '(', '”', 'q', 'd', 'i', 'f', ',', 'h', 'b', ':', 'v', '7', '/', '&', 'k', 'x', 't', '.', '1', ']', '?', ';', '–', '…', 'p', 'g', '\\n', 'o', ' ', '!', 'j', '“', '$', '3', '4', 'r', 'e', '\"', 'm', 'u', 'é', '%', '’', 's', 'z', '[', '-', '9', '\\ufeff', ')', '‘', '@', \"'\", '6'}\n",
            "Length of Unique characters: 67\n",
            "Number of characters in data: 896270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JeZxwLsB3had",
        "outputId": "1ce1e439-0fa9-4270-d35f-377cc2a0cce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'l': 0, '5': 1, 'a': 2, '8': 3, '=': 4, 'w': 5, 'y': 6, '0': 7, 'c': 8, 'n': 9, '—': 10, '2': 11, '_': 12, '(': 13, '”': 14, 'q': 15, 'd': 16, 'i': 17, 'f': 18, ',': 19, 'h': 20, 'b': 21, ':': 22, 'v': 23, '7': 24, '/': 25, '&': 26, 'k': 27, 'x': 28, 't': 29, '.': 30, '1': 31, ']': 32, '?': 33, ';': 34, '–': 35, '…': 36, 'p': 37, 'g': 38, '\\n': 39, 'o': 40, ' ': 41, '!': 42, 'j': 43, '“': 44, '$': 45, '3': 46, '4': 47, 'r': 48, 'e': 49, '\"': 50, 'm': 51, 'u': 52, 'é': 53, '%': 54, '’': 55, 's': 56, 'z': 57, '[': 58, '-': 59, '9': 60, '\\ufeff': 61, ')': 62, '‘': 63, '@': 64, \"'\": 65, '6': 66}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7jWk2xfz3hap",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dvV4rxVf3ha2",
        "outputId": "19100322-74cf-4127-c531-f2dd65f0bbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[61, 56, 37, 49, 49, 8, 20, 41, 31, 39, 39, 39, 30, 30, 30, 29, 20, 2, 9, 27, 41, 6, 40, 52, 41, 56, 40, 41, 51, 52, 8, 20, 30, 41, 41, 29, 20, 2, 29, 65] 40 12406\n",
            "[56] 1 12406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bc33tzci3hbH",
        "outputId": "f784f693-5ef1-4747-f898-0a3b8bace119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: ﻿speech 1\n",
            "\n",
            "\n",
            "...thank you so much.  that'\n",
            "Output: s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fdf6OwEc3hbZ",
        "outputId": "ea304659-eb54-4c2c-d751-432828a07f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[61 56 37 49 49  8 20 41 31 39 39 39 30 30 30 29 20  2  9 27 41  6 40 52\n",
            " 41 56 40 41 51 52  8 20 30 41 41 29 20  2 29 65] (12406, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4fdSGiVe3hbo",
        "outputId": "7cba0284-a888-4b6a-ad0e-693595234f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (12406, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RjNkKwiu3hcA",
        "outputId": "243f4ca1-cd30-4fd3-8085-a8a9b705968e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (9924, 40) (9924, 67)\n",
            "Validation: (2482, 40) (2482, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KOpgGj6H3hcQ",
        "outputId": "3caf41c0-dde0-4935-df7e-472fecfd69c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 40, 512)           34304     \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_18 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 67)                34371     \n",
            "=================================================================\n",
            "Total params: 4,267,075\n",
            "Trainable params: 4,267,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tbEL0O7A3hca",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ky6mgb7o3hcj",
        "outputId": "fe039ca5-525f-4303-a71e-5b6064e2deb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9924 samples, validate on 2482 samples\n",
            "Epoch 1/150\n",
            "9924/9924 [==============================] - 42s 4ms/step - loss: 3.1119 - val_loss: 2.8270\n",
            "Epoch 2/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 2.6897 - val_loss: 2.4695\n",
            "Epoch 3/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 2.4436 - val_loss: 2.2902\n",
            "Epoch 4/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 2.3029 - val_loss: 2.1742\n",
            "Epoch 5/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 2.1802 - val_loss: 2.0838\n",
            "Epoch 6/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 2.0900 - val_loss: 2.0139\n",
            "Epoch 7/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 2.0052 - val_loss: 1.9529\n",
            "Epoch 8/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 1.9018 - val_loss: 1.9402\n",
            "Epoch 9/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 1.8186 - val_loss: 1.9030\n",
            "Epoch 10/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 1.7483 - val_loss: 1.8908\n",
            "Epoch 11/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 1.6727 - val_loss: 1.8733\n",
            "Epoch 12/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 1.6029 - val_loss: 1.8956\n",
            "Epoch 13/150\n",
            "9924/9924 [==============================] - 38s 4ms/step - loss: 1.5093 - val_loss: 1.9079\n",
            "Epoch 00013: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe28c4bf3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "r3_hvcqx3hcy",
        "outputId": "a0a03a7f-a2f8-4de4-babd-fb36370c17a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" me, i am a unifier. once we get all of t \"\n",
            "\n",
            "\n",
            "me, i am a unifier. once we get all of thinks have wonve honey risatigiaidn.\n",
            "you know, an peolle have 6s nut.\n",
            "i know, this ase, many bery nigf stroa trbat. you yea. what hele goed to this he5’re buese where the froon cranys welve ad’s jven’s contry.\n",
            "morly this nveos with reose that be shesd and chat was 1100. you. goo’s looc stuf. the. yibe woull don’is being holle. not the wiedd chank ane hon 8f silieica'e alpony but uns thisks and thighapactuip,  umher \n",
            "ow, pucaess. stals any i ving in slet's of the peoplius.\n",
            "yo buse going\n",
            "that ’t going to we paks that.\n",
            "\" has wher asering becasic. whey dacans hibe, peolle do is – out whinh and a certing ad asaony puoricichs chonkend.\n",
            " we \"mave, i miding boing upolidive lidve jrost 6f that-\n",
            "se chean we les?\n",
            "theif realbes. do ope the dass it beycion.\n",
            "\"ou goen thas knows,.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hom so peolly there aid.\n",
            "you know but is seois.\n",
            ".\n",
            "ou be, i’m of 9oe.. an crank. he have to this is gos anlysadive sbigitiob people mucku. r\n",
            "ksoug.\" and sousn wore a milted of hersatar, we cin’t – i’m going to that.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EO9mfWQE3hc8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Olu_UhfK4Lnl"
      },
      "cell_type": "markdown",
      "source": [
        "## AnnaRNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d64_YY2Q4Lnu",
        "outputId": "2f905aec-8bb9-49a4-ab93-68da0ebc4993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = 'data/anna.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "  \n",
        "    data = f.read().lower()\n",
        "    \n",
        "print ('First 200 characters of data\\n')\n",
        "print (data[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 200 characters of data\n",
            "\n",
            "chapter 1\n",
            "\n",
            "\n",
            "happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "everything was in confusion in the oblonskys' house. the wife had\n",
            "discovered that the husband was carrying on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K-A3fbSn4LoE",
        "outputId": "fa738482-d37b-4f15-fc02-712f0189d672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# unique characters in dataset and its count\n",
        "chars = set(data)\n",
        "vocab_size = len(set(data))\n",
        "data_size = len(data)\n",
        "print ('Unique characters:', chars)\n",
        "print ('Length of Unique characters:', vocab_size)\n",
        "print ('Number of characters in data:', data_size)\n",
        "\n",
        "#restricting so that we can load all data onto RAM\n",
        "#uncomment this line to run on all data\n",
        "data = data[:585223]\n",
        "data_size = len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: {'l', '5', 'a', '8', 'w', 'y', '0', 'c', 'n', '_', '2', '(', 'q', 'd', 'i', 'f', ',', 'h', 'b', ':', 'v', '7', '/', '&', 'k', 'x', 't', '.', '1', '?', ';', 'p', 'g', '\\n', 'o', ' ', '!', 'j', '3', '$', '4', 'r', 'e', '\"', 'm', 'u', '*', '%', '`', 's', 'z', '-', '9', ')', '@', \"'\", '6'}\n",
            "Length of Unique characters: 57\n",
            "Number of characters in data: 1985223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l5Dz5QvX4LoP",
        "outputId": "c45492cf-2a2e-4f2a-f3a7-e9154413633f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "char2id = {ch:i for i, ch in enumerate(chars)}\n",
        "id2char = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print ('Characters to id\\n')\n",
        "print (char2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters to id\n",
            "\n",
            "{'l': 0, '5': 1, 'a': 2, '8': 3, 'w': 4, 'y': 5, '0': 6, 'c': 7, 'n': 8, '_': 9, '2': 10, '(': 11, 'q': 12, 'd': 13, 'i': 14, 'f': 15, ',': 16, 'h': 17, 'b': 18, ':': 19, 'v': 20, '7': 21, '/': 22, '&': 23, 'k': 24, 'x': 25, 't': 26, '.': 27, '1': 28, '?': 29, ';': 30, 'p': 31, 'g': 32, '\\n': 33, 'o': 34, ' ': 35, '!': 36, 'j': 37, '3': 38, '$': 39, '4': 40, 'r': 41, 'e': 42, '\"': 43, 'm': 44, 'u': 45, '*': 46, '%': 47, '`': 48, 's': 49, 'z': 50, '-': 51, '9': 52, ')': 53, '@': 54, \"'\": 55, '6': 56}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kTLv9Yc44LoX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cut the text into fixed size inputs of length maxlen\n",
        "maxlen = 40\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "end = data_size - maxlen\n",
        "for i in range(0, end, maxlen):\n",
        "    sentences.append([char2id[ch] for ch in data[i : i+maxlen]])\n",
        "    next_chars.append([char2id[ch] for ch in data[i+maxlen]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tnYEPM0s4Loh",
        "outputId": "cde5e9e0-0c29-4e6c-e707-74d1962a0fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "print (sentences[0], len(sentences[0]), len(sentences))\n",
        "print (next_chars[0], len(next_chars[0]), len(next_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7, 17, 2, 31, 26, 42, 41, 35, 28, 33, 33, 33, 17, 2, 31, 31, 5, 35, 15, 2, 44, 14, 0, 14, 42, 49, 35, 2, 41, 42, 35, 2, 0, 0, 35, 2, 0, 14, 24, 42] 40 14630\n",
            "[30] 1 14630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4GsUgjOK4Lo3",
        "outputId": "9118a914-c77f-477e-a472-14484350db72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Input:', ''.join(id2char[i] for i in sentences[0]))\n",
        "print ('Output:', id2char[next_chars[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: chapter 1\n",
            "\n",
            "\n",
            "happy families are all alike\n",
            "Output: ;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PsHJoEAz4LpF",
        "outputId": "f71a19ce-105f-486c-b51c-986f4f06b90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
        "X = np.array(sentences).reshape(X.shape)\n",
        "print (X[0], X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 7 17  2 31 26 42 41 35 28 33 33 33 17  2 31 31  5 35 15  2 44 14  0 14\n",
            " 42 49 35  2 41 42 35  2  0  0 35  2  0 14 24 42] (14630, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FMv01gA24Lpd",
        "outputId": "136585d4-f1aa-4ba4-9988-3825814dc567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "Y = np.zeros((len(next_chars), vocab_size), dtype=np.float32)\n",
        "one_hot = [to_categorical(c, num_classes=vocab_size) for i in range(len(next_chars)) for c in next_chars[i]]\n",
        "Y = np.array(one_hot).reshape(Y.shape)\n",
        "print (Y[0], Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.] (14630, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DuXFciDp4Lpp",
        "outputId": "5578efdc-b3ee-432a-bb14-afcde4cc290b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size=0.2)\n",
        "print ('Training:', train_x.shape, train_y.shape)\n",
        "print ('Validation:', val_x.shape, val_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (11704, 40) (11704, 57)\n",
            "Validation: (2926, 40) (2926, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fHDbxX1L4Lp2",
        "outputId": "16df8cb5-7395-4b83-9931-a18c429a5091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512, input_length=maxlen))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 40, 512)           29184     \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 57)                29241     \n",
            "=================================================================\n",
            "Total params: 4,256,825\n",
            "Trainable params: 4,256,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BBIRaK9F4Lp_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(data) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.7, 1.3]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = data[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen))\n",
        "            one_hot = [char2id[c] for c in sentence]\n",
        "            x_pred = np.array(one_hot).reshape(x_pred.shape)\n",
        "\n",
        "            preds = model.predict(x_pred)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = id2char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mUt4LVnn4LqI",
        "outputId": "68491a47-adcf-4057-ec40-5837c5cfb7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "# use print_callback to print for every epoch\n",
        "# model.fit(train_x, train_y,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(val_x, val_y),\n",
        "#           callbacks=[print_callback, es])\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(val_x, val_y),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11704 samples, validate on 2926 samples\n",
            "Epoch 1/150\n",
            "11704/11704 [==============================] - 49s 4ms/step - loss: 3.0676 - val_loss: 2.7058\n",
            "Epoch 2/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 2.5816 - val_loss: 2.4042\n",
            "Epoch 3/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 2.4086 - val_loss: 2.2692\n",
            "Epoch 4/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 2.2893 - val_loss: 2.1879\n",
            "Epoch 5/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 2.1972 - val_loss: 2.1219\n",
            "Epoch 6/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 2.1055 - val_loss: 2.0752\n",
            "Epoch 7/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 2.0238 - val_loss: 2.0388\n",
            "Epoch 8/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 1.9492 - val_loss: 2.0120\n",
            "Epoch 9/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 1.8711 - val_loss: 2.0153\n",
            "Epoch 10/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 1.7901 - val_loss: 1.9999\n",
            "Epoch 11/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 1.7272 - val_loss: 1.9928\n",
            "Epoch 12/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 1.6426 - val_loss: 2.0183\n",
            "Epoch 13/150\n",
            "11704/11704 [==============================] - 44s 4ms/step - loss: 1.5864 - val_loss: 2.0296\n",
            "Epoch 00013: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe28b19d400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "csLb-xzU4LqT",
        "outputId": "b20f4796-11b5-4e71-b96b-eb6a9857d480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(sentences)-1)\n",
        "pattern = sentences[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")\n",
        "sampled = [id2char[value] for value in pattern]\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "  \n",
        "    x = np.zeros((1, maxlen), dtype=np.int32)\n",
        "    x = np.array(pattern).reshape(x.shape)\n",
        "    \n",
        "    prediction = model.predict(x)[0]\n",
        "\n",
        "    index = sample(prediction, 1.2)\n",
        "    result = id2char[index]\n",
        "\n",
        "    #sys.stdout.write(result)\n",
        "    sampled.append(result)\n",
        "    \n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\n\")\n",
        "print (''.join(s for s in sampled))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  the preparatory\n",
            "bustle in the station,  \"\n",
            "\n",
            "\n",
            " the preparatory\n",
            "bustle in the station, becusion of a come the that i's soos, ebmad .vsen, and qucaring shispavy\n",
            "roull can\" his\n",
            "shasce gill, qulwigill; and, and same to it a lich, and loke, in the craot, and tither.\n",
            "\n",
            "\"an'd pave rood, mad\n",
            "yow roming simrony and bufsens-at the maering which of hamyebmlely\n",
            "\"vevy dedi ut miall wiwh pustady rompiviver-q,\n",
            "chogsht, ches! i' dlolming, desding the coud her, myever.\n",
            "\n",
            "\"why _he's sleuving,\" she\n",
            "kace like come\n",
            "onethersigpzy prisice, and and the gordian, the baclliratarriin.\n",
            "\"\"ls, you ruke go nor visief, would on mavin undersanty fror the coolbnorihe, and haven and wondeds, lecutting a heorgg when, shikle\n",
            "the said\n",
            "to memnevher?\n",
            "she gels, but over whe, he rapped? whit he cinter, it i bistrele.\n",
            "\n",
            "\"\"ame notf that qeiuntera'nts, acruls a prossed no her\n",
            "menslous, meas docince, whan athing. \n",
            "but for that the vovbed the shourd nov a neble on the couct detting; to her souly exlouckly on the munces of vorige yo..\n",
            "\n",
            "\"\"ho, we lvan ky swap, and boce it it ich wof the yoor murtom a mouml..\n",
            "\n",
            "\"oh, and kat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uztxtqeJ4Lq6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}